
\documentclass[12pt]{report}

%  PACKAGES ==========================================================
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd,mathrsfs,mathtools}  % Math packages
\usepackage{natbib}         % for references
\usepackage{multirow}       % for equaitons
\usepackage{wrapfig}        % to wrap figure with text
\usepackage{graphicx}       % allows adding graphics to the text
\usepackage{verbatim}       % Allows quoting source with commands.
\usepackage{makeidx}        % Package to make an index.
%\usepackage{psfig}          % Allows inclusion of eps files.
%\usepackage{epsfig}         % Allows inclusion of eps files.
%\usepackage{epstopdf}       % allows .eps figures in the text
%\usepackage{pdftex}        % allows .eps figures in the text
%\usepackage{pdflatex}      % allows .eps figures in the text

\usepackage{url}            % Allows good typesetting of web URLs.
\usepackage{subfigure}      % to have multi figures in the same figure
\usepackage{comment}        % allow commnets in the file
%\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e} % To include algorithm - this one is more professional, but we cannot break the algorithm in pages
\usepackage{float}
%\usepackage{algorithm}      % To include algorithm 
%\usepackage{algpseudocode}  % for algorithms 
\usepackage[bookmarks]{hyperref}  % to add bookmarks to all references
\usepackage{ragged2e}       % Text justifying
\usepackage{color}
\usepackage{ctable}
%\usepackage{pmat}
%\usepackage{ulem}
%\usepackage[retainorgcmds]{IEEEtrantools}
%\usepackage[dvips]{geometry}
%\usepackage{lscape}
%\setcounter{MaxMatrixCols}{40}

%\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode} % Add a Matlab code
%\usepackage{eucal} 		    % Euler fonts
%\usepackage{setspace}
%\usepackage{algorithm,algorithmicx,algpseudocode}   % algorithms
%\usepackage{draftcopy}		% Uncomment this line to have the
                          %   word, "DRAFT," as a background
                          %   "watermark" on all of the pages of
                          %   of your draft versions. When ready
                          %   to generate your final copy, re-comment
                          %   it out with a percent sign to remove
                          %   the word draft before you re-run
                          %   Makediss for the last time.

% Extra stuff =======================================================
%\allowdisplaybreaks  % to break long equations into two pages


% = Constants ========================================================
\renewcommand\textfraction{.1}
\setcounter{totalnumber}{5}
\renewcommand{\floatpagefraction}{.8}

%\setlength{\bibsep}{0pt plus 0.3ex}    % This line reduces the spacing between the references  - Remove in the final version

% = Figure spacing ==================================================
\renewcommand{\subfigbottomskip}{-2pt}
\renewcommand{\subfigcapskip}{-2pt}
\renewcommand{\subfigcapmargin}{0pt}

%\setlength{\belowcaptionskip}{-10pt}

\setlength{\textfloatsep}{14pt plus 1.0pt minus 2.0pt}
\setlength{\intextsep}{14pt plus 1.0pt minus 2.0pt}
\setlength{\floatsep}{14pt plus 1.0pt minus 2.0pt}


% General information ===============================================
\author{Babak Poursartip}
%\address{babakp@utexas.edu}

\title{My ML studies}

\makeatletter		% Starts section where @ is considered a letter
          			% and thus may be used in commands.
\def\square{\RIfM@\bgroup\else$\bgroup\aftergroup$\fi
  \vcenter{\hrule\hbox{\vrule\@height.6em\kern.6em\vrule}%
                                              \hrule}\egroup}
\makeatother		% Ends sections where @ is considered a letter.
			          % Now @ cannot be used in commands.

%\makeindex      % Make the index


% Beginning =========================================================
\begin{document}

\justifying


%\titlepage    % Produces the title page.
\maketitle



% Tables ============================================================
\pdfbookmark{\contentsname}{toc}  % To bookmark the table of contents
\tableofcontents                  % Table of Contents will be automatically
%\listoftables                     % List of Tables and List of Figures will be placed
%\listoffigures                    % here, if applicable.
%\listofalgorithms                 % List of algorithms
%\addcontentsline{toc}{chapter}{List of Algorithms}  % This command includes the list of algorithms in the table of content

% Chapters =====================================================================
%===============================================================================
%===============================================================================
%===============================================================================
%\include{chapter1_Introduction}
\chapter[Introduction]{Introduction to ML}
\label{ch:ML}
\noindent

\section{General}

\begin{itemize}
	\item Supervised learning (SL): functional approximation from labeled data. 
	\item Unsupervised learning (UL): functional description. Learning from unlabeled data. 
	\item Reinforcement learning (RL): Learning from delayed reward. 
\end{itemize}






%===============================================================================
%===============================================================================
%===============================================================================
\chapter[Supervised learning]{Supervised learning (SL)}
\label{ch:sl}
There are two types of supervised learning:

\begin{itemize}
\item classification: taking some inputs and mapping it to some discrete labels. (true of false; male or female; SUV or sedan or truck; class1, class2, class3.)
\item regression: returns continuous valued function. Mapping from input to some \textbf{real number}. Something like age is more like discrete number, so, it is also classification. 
\end{itemize}

\section{Terms}
\begin{itemize}
\item Instances: Inputs, such as pictures, pixels, etc.
\item Concept: A set of \textit{functions} that maps inputs to outputs.
\item Target concept: The actual function that can map inputs to outputs. This is the actual answer.
\item Hypothesis class: Set of all the functions that we are going to think about. 
\item Sample (Training set): A set of instances with correct labels.
\item Candidate: The best approximation of the target concept.
\item Testing set: A set of instances with correct labels that was not visible to the learning algorithm during the training phase. It’s used to determine the algorithm performance on novel data. we can argue that we learned something by memorization, but indeed, we just memorized the concepts. What we want is generalization. 
\end{itemize}


\section{SL: Decision tree}

\subsection{Introduction}

Decision tree is a classification learning in the form of a sequence of decisions based on the attributes/features/questions (which forms the nodes) applied to every instance to assign it to a specific class. Answers to the questions would be the edges of the trees.
For example, deciding to eat at a restaurant or not, or classification of vehicles into sedans, SUVs, trucks, etc.

\subsection{Representation of the decision tree:}

A decision tree is a structure of nodes, edges and leaves that can be used to \textit{represent} the data. We always start at the root of the tree, otherwise we don't look at any other attribute in the tree. 
\begin{itemize}
\item Nodes represent attributes where you ask a question about it. (vehicle length, vehicle height, number of doors, etc.).
\item Edges represent values/answers, a specific value for each attribute/question.
\item Leaves represent the output (or final answer). For example, vehicle’s type.
\end{itemize}

\subsection{Algorithm to build a decision tree}
Algorithm means how to create the decision tree. A decision tree \textit{algorithm} is a sequence of steps that will lead you to the desired output. To form a decision tree, we need to come up with the attributes that can split the space, roughly in half. 
\begin{itemize}
\item Pick the best attribute (the one that can split the data roughly in half). If this attribute added no valuable information (not a good split), it might cause overfitting.
\item Ask a question about this attribute.
\item Follow the correct answer path.
\item Loop back to (1) till you narrow down the possibilities to one answer (the output).
\end{itemize}
Note that the usefulness of a question depends upon the answers we got to previous questions.

\subsection{Expressiveness}

\begin{itemize}
	\item Decision trees can basically express any function using the input attributes.
	\item For Boolean functions (AND, OR, XOR, etc.), the truth table row will be translated into a path to a leaf.
	\item How many decision trees we can generate for a specific function/problem? For n boolean attributes with boolean output, the decision tree hypothesis space is very huge $(2^{(2^{n})})$. (Babak note:  this number of decision tree is not really distinct, see my notebook).
	This is why we need to design algorithms to efficiently search the hypothesis space for the best decision tree. 
\end{itemize}

Decision trees with AND and OR are linear (they need linear number of nodes-ANY type problems). But XOR is a hard problem (requires $2^n$ nodes-PARITY type of problems).

\subsection{ID3 algorithm to create a decision tree}
We need to find the best attributes for each node, to narrow down the space with each question. 
ID3 (Inducing Decision Tree (3)) builds decision trees using a top-down, greedy approach. The greedy part of the approach comes from the fact that it will decide which attribute should be at the root of the tree by looking just one move ahead. It compares all available attributes to find which one classifies the data the best, but it doesn't look ahead (or behind) at other attributes to see which combinations of them classify the data the best. ID3 algorithm returns an optimal decision tree, but as the size of the training data and the number of attributes increase, it becomes more likely that running ID3 on it will return a suboptimal decision
tree.

Pseudo code:
\begin{enumerate}
\item Pick the best attribute A (the definitions of best attribute comes later). To select this attribute, a statistical test is used to determine for each attribute how well it alone classifies the training examples.
\item Split the data into subsets that correspond to the possible values of the best attribute.
\item Assign A as a decision attribute for a node.
\item For each value of A, create a descendant node.
\item Sort training examples to leaves.
\item If examples perfectly classified (means all data point are of the same class) or there is no more attributes --	Stop -- else --Iterate over leaves	
\end{enumerate}	


How to find the best attribute for the decision tree at each level: \\
There are a couple of options. The most common method is called information gain: a measure that expresses how well an attribute splits the data into groups based on classification. For example, an attribution returns \textit{low information gain}, if it divides samples to two classes with an even yes and no, but the information gain is high, if each class has more of yeses or nos.

It quantifies the reduction in randomness (Entropy) over the labels we have with a set of data, based upon knowing the value of a particular attribute. A mathematical way to measure the gain, is entropy. It measures the homogeneity of a data set S's  classifications. \textcolor{red}{\textit{Entropy ranges from 0, which means that all of the classifications in the data set are the same (either yes or no), to $log_{2}$ of the number of different classifications, which means that the classifications are equally distributed within the data set.}} For a binary classification the entropy is only $log_{2} 1 = 1$ (if yes and no samples are equally divided). entropy is calculated as follows: 

\begin{equation}
Entropy(S) = \sum_{i=1}^{c} -p_{i} \; log_{2} (p_{i})
\end{equation}
where:
\begin{itemize}
\item $c$ corresponds to the number of different classifications (either for the attribute or the final output)
\item $p_{i}$ corresponds to the proportion of the data with the classification i
\end{itemize}

Here is the plot of entropy for a binary classifier, as the proportional of yes and no samples change:

\includegraphics[scale=0.5]{entroy.png}

Information gain measures the reduction in entropy that results from partitioning the data on an attribute A, which is another way of saying that it represents how effective an attribute is at classifying the data. Given a set of training data S and an attribute A, the formula for information gain is:

\begin{equation}
	Gain(S,A)  = Entropy(S) - \sum_v  \frac{|S_v|}{|S|} \; Entropy(S_v)
\end{equation}


where $v$ is all the possible values of attribute the attribute (for example, sunny, cloudy, rainy), and $S_v$ is the total number of samples corresponding to this value of attribute (for example sunny). 

For \textit{if I play tennis game} (yes and no is the final output) with attributes weather (sunny, cloudy, rainy), temperature (hot, cold, mild), humidity (normal, high),  first, we calculate the entropy of the entire set (Entropy(S)) (wrt the the final output classification, yes and no). For each attribute, we calculate the the entropy corresponding to each value of entropy, then we calculate the final entropy. 


We want to maximize information gain, so we want the entropies of the partitioned data to be as low as possible, which explains why attributes that exhibit high information gain split training data into relatively heterogeneous groups.

As the size of the training data and the number of attributes increase, it becomes likelier that running ID3 on it will return a sub-optimal decision tree.

\subsection{Inductive bias}
Two types of bias for classifiers: 
\begin{itemize}
	\item Restriction Bias: Describes the hypothesis set space (H) that we will consider for learning the tree.
	Complete hypothesis space (low Restriction Bias). Instead of looking at infinitely many functions, we only consider those that can be represented by the decision tree. 
	\item Preference Bias: Describes the subset of hypothesis n ($n \in H $), from the hypothesis set space H, that we prefer.
\end{itemize}

Inductive bias of ID3 algorithm:
\begin{itemize}
\item it prefers the decision tree with good splits at top (even if a bad split generates the same outcome). This is because ID3 is greedy using info gain. 
\item it prefers correct outcome over incorrect because ID3 repeats until the labels are correctly classified.
\item it prefers shorter trees (comes from the fact that we use good splits from the top).
\end{itemize}

\subsection{Extending ID3 algorithm-other consideration}
\textbf{For continuous attributes}, we can classify the attributes based on thresholds (that exists in the data set). The continuous data attribute provides the most information gain, while giving a decision tree that is not generalize well. 

Does it make sense to repeat an attribute along a path in a decision tree? No, it does not make sense for discrete values, but for continuous values, it makes sense, because indeed we are asking a different question (for a different range). 

If the data for some attributes is missing, we can choose the most popular data for the missing item. Two options here: does not take the classification into account, or choose the most popular value for the same classification. 
Another method to deal with the missing data, is assigning the probability of each value of the attribute to the missing entries. 

\textbf{When do we stop?} When everything classified correctly or if there is no more attribute. What if  we have noise in data (different answer for the same instance)? Causes an infinite loop. What would be the stopping criterion, then? We want generalization, and we should avoid \textbf{over-fitting}. If the tree is too big/complicated, there is a chance that it overfits. There are two popular approaches to avoid over-fitting in decision trees: stop growing the tree before it becomes too large or prune the tree after it becomes too large. Typically, a limit to a decision tree’s growth will be specified in terms of the maximum number of layers, or depth, it’s allowed to have. 


One option to avoid over-fitting, is that we can grow the trees, and use cross-validation to prevent over-fitting. The data available to train the decision tree will be split into a training set and validation/test set and we keep growing the tree with various maximum depths will be created based on the training set and then test it against the test set. The best will be selected (when the error grows, we stop growing the tree).

Pruning the tree, on the other hand, involves testing the original tree against pruned versions of it. Leaf nodes are taken away from the tree as long as the pruned tree performs better against test data than the larger tree.


\textbf{Regression:} how to adapt decision trees for regression type of problems (continuous outputs)? Decision trees as we’ve defined them here don’t transfer directly to regression problems. We no longer have a useful notion of information gain, so our approach at attribute sorting falls through. Instead, we can rely on purely statistical methods (like variance and correlation) to determine how important an attribute is. For leaves, too, we can do averages, local linear fit, or a host of other approaches that mathematically generalize with no regard for the meaning of the data.

\subsection{Changing the information gain formula is another option}
The information gain formula used by the ID3 algorithm treats all of the variables the same, regardless of their distribution and their importance. This is a problem when it comes to continuous variables or discrete variables with many possible values because training examples may be few and far between for each possible value, which leads to low entropy and high information gain by virtue of splitting the data into small subsets, but results in a decision tree that might not generalize well.

One successful approach to deal with this is using a formula called GainRatio in the place of information gain. GainRatio tries to correct for information gain’s natural bias toward attributes with many possible values by adding a denominator to information gain called SplitInformation. SplitInformation attempts to measure how broadly partitioned an attribute is and how evenly distributed those partitions are. In general, the SplitInformation of an attribute with n equally ­distributed values is $log_2 n$ . These relatively large denominators significantly affect an attribute’s chances of being the best attribute after an iteration of the ID3 algorithm and help to avoid choices that perform particularly well on the training data but not so well outside of it.

\begin{gather*}
	GainRatio(S,A)  = \frac{Gain(S,A)}{SplitInformation(S,A)} \\
	SplitInformation(S,A) = - \sum_{i=1}^c \frac{|S_i|}{S} \; log_2\frac{|S_i|}{|S|}
\end{gather*}


\subsection{Advantages and Disadvantages of Decision Trees (copied from a text)}
\textbf{Advantages:}

\begin{itemize}
\item Simple to understand and to interpret.
\item Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed.
\item The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.
\item Able to handle both numerical and categorical data. Other techniques are usually specialized in analyzing datasets that have only one type of variable.
\item Able to handle multi-output problems.
\item Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by Boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.
\item Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.
\item Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.
\end{itemize}



\textbf{Disadvantages:}

\begin{itemize}
\item Can create over-complex trees that do not generalize the data well. This is called overfitting. Mechanisms such as pruning (setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree) are necessary to avoid this problem.
\item Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.
\item The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally
optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.
\item There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.
\item Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.
\end{itemize}


\subsection{Random forest}
The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters. If you keep modeling, you can learn more models with even better performance, but many of those are sensitive to getting the right parameters.



\section{SL: Regression and classification}
\section{SL: Neural network}
\section{SL: Instance based learning}
\section{SL: Ensemble B\&B}
\section{SL: Kernel methods and SVMs}
\section{SL: Comp Learning Theory}
\section{SL: VC dimensions}
\section{SL: Bayesian Learning}
\section{SL: Bayesian inference}


%===============================================================================
%===============================================================================
%===============================================================================
\chapter[Unsupervised learning]{Unsupervised learning (UL)}

\section{Random optimization}
\section{Clustering}

\chapter[Reinforced Learning]{Reinforced Learning (RL)}

\section{Markov Decision Process}

\chapter[Miscellaneous topics]{Miscellaneous topics}

\section{One hot encoding}

\href{https://www.youtube.com/watch?v=v_4KWmkwmsU&ab_channel=deeplizard}{source1} and \href{https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/}{source2}

A one hot encoding is a representation of categorical variables as binary vectors. This first requires that the categorical values be mapped to integer values. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1. 

Example: assume we have a sequence of labels with the values \textit{red} and \textit{green}. We can assign \textit{red} an integer value of 0 and \textit{green} the integer value of 1. As long as we always assign these numbers to these labels, this is called an integer encoding. Consistency is important so that we can invert the encoding later and get labels back from integer values, such as in the case of making a prediction. Next, we can create a binary vector to represent each integer value. The vector will have a length of 2 for the 2 possible integer values. The \textit{red} label encoded as a 0 will be represented with a binary vector [1, 0] where the zeroth index is marked with a value of 1. In turn, the \textit{green} label encoded as a 1 will be represented with a binary vector [0, 1] where the first index is marked with a value of 1. If we had the sequence: \textit{red, red, green}, we could represent it with the integer encoding: 0, 0, 1. And the one hot encoding of:
[1, 0], [1, 0], and [0, 1].

Why Use a One Hot Encoding?
A one hot encoding allows the representation of categorical data to be more expressive. Many machine learning algorithms cannot work with categorical data directly. The categories must be converted into numbers. This is required for both input and output variables that are categorical. We could use an integer encoding directly, rescaled where needed. This may work for problems where there is a natural ordinal relationship between the categories, and in turn the integer values, such as labels for temperature \textit{cold, warm, and hot}.

There may be problems when there is no ordinal relationship and allowing the representation to lean on any such relationship might be damaging to learning to solve the problem. An example might be the labels dog and cat

In these cases, we would like to give the network more expressive power to learn a probability-like number for each possible label value. This can help in both making the problem easier for the network to model. When a one hot encoding is used for the output variable, it may offer a more nuanced set of predictions than a single label.

We can use libraries such is scikit-learn and keras to encode a categorical feature for ML.

\section{F1 score, precision, and recall}
Refer to \href{https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#precision-recall}{this source.}

Precision (P) is defined as the number of true positives ($T_p$) over the number of true positives plus the number of false positives ($F_p$):
$P = \frac{T_p}{T_p + F_p} $.

Recall    (R) is defined as the number of true positives ($T_p$) over the number of true positives plus the number of false negatives ($F_n$):
$R = \frac{T_p}{T_p + F_n} $.

These quantities are also related to the ($F1$) score, which is defined as the harmonic mean of precision and recall:
$F1 = 2 \frac{R * P}{ P + R } $.

The F1 score can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.


\section{Overfitting vs underfitting}
This is a phenomenon called overfitting, where a model matches the training data almost perfectly, but does poorly in validation and other new data. On the flip side, if we make our tree very shallow, it doesn't divide up the houses into very distinct groups.

At an extreme, if a tree divides houses into only 2 or 4, each group still has a wide variety of houses. Resulting predictions may be far off for most houses, even in the training data (and it will be bad in validation too for the same reason). When a model fails to capture important distinctions and patterns in the data, so it performs poorly even in training data, that is called underfitting.

Since we care about accuracy on new data, which we estimate from our validation data, we want to find the sweet spot between underfitting and overfitting. Visually, we want the low point of the (red) validation curve in the figure below.

% Appendix ==========================================================
%\appendices
%\include{appendix_Matrices}


%\renewcommand*{\bibfont}{\footnotesize}  % This command reduces the font size. Comment this for the final draft
%\renewcommand*{\bibfont}{\scriptsize}  % This command reduces the font size. Comment this for the final draft
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{plain}  				% Here the bibliography  %
\bibliography{reference}        	% is inserted.			     %
\index{bibliography}

\end{document}

