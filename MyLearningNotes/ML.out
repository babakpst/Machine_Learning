\BOOKMARK [0][-]{toc.0}{Contents}{}% 1
\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 2
\BOOKMARK [1][-]{section.1.1}{General}{chapter.1}% 3
\BOOKMARK [0][-]{chapter.2}{Supervised learning}{}% 4
\BOOKMARK [1][-]{section.2.1}{Terms}{chapter.2}% 5
\BOOKMARK [1][-]{section.2.2}{SL: Decision tree}{chapter.2}% 6
\BOOKMARK [2][-]{subsection.2.2.1}{Introduction}{section.2.2}% 7
\BOOKMARK [2][-]{subsection.2.2.2}{Representation of the decision tree:}{section.2.2}% 8
\BOOKMARK [2][-]{subsection.2.2.3}{Algorithm to build a decision tree}{section.2.2}% 9
\BOOKMARK [2][-]{subsection.2.2.4}{Expressiveness}{section.2.2}% 10
\BOOKMARK [2][-]{subsection.2.2.5}{ID3 algorithm to create a decision tree}{section.2.2}% 11
\BOOKMARK [2][-]{subsection.2.2.6}{Inductive bias}{section.2.2}% 12
\BOOKMARK [2][-]{subsection.2.2.7}{Extending ID3 algorithm-other consideration}{section.2.2}% 13
\BOOKMARK [2][-]{subsection.2.2.8}{Regression with Decision tree}{section.2.2}% 14
\BOOKMARK [2][-]{subsection.2.2.9}{Changing the information gain formula is another option}{section.2.2}% 15
\BOOKMARK [2][-]{subsection.2.2.10}{Advantages and Disadvantages of Decision Trees \(copied from a text\)}{section.2.2}% 16
\BOOKMARK [2][-]{subsection.2.2.11}{When to use a decision tree}{section.2.2}% 17
\BOOKMARK [2][-]{subsection.2.2.12}{Random forest \(Tree ensembles\)}{section.2.2}% 18
\BOOKMARK [2][-]{subsection.2.2.13}{Boosted tree \(XGBoost\)}{section.2.2}% 19
\BOOKMARK [1][-]{section.2.3}{SL: Regression and classification}{chapter.2}% 20
\BOOKMARK [2][-]{subsection.2.3.1}{Linear regression}{section.2.3}% 21
\BOOKMARK [2][-]{subsection.2.3.2}{Non-linear/polynomial regression}{section.2.3}% 22
\BOOKMARK [2][-]{subsection.2.3.3}{Error}{section.2.3}% 23
\BOOKMARK [2][-]{subsection.2.3.4}{Multiple Linear regression \(multiple features\)}{section.2.3}% 24
\BOOKMARK [2][-]{subsection.2.3.5}{Other cases}{section.2.3}% 25
\BOOKMARK [2][-]{subsection.2.3.6}{Classification}{section.2.3}% 26
\BOOKMARK [1][-]{section.2.4}{SL: Neural network}{chapter.2}% 27
\BOOKMARK [2][-]{subsection.2.4.1}{Perceptron}{section.2.4}% 28
\BOOKMARK [1][-]{section.2.5}{SL: Instance based learning}{chapter.2}% 29
\BOOKMARK [1][-]{section.2.6}{SL: Ensemble B\046B}{chapter.2}% 30
\BOOKMARK [1][-]{section.2.7}{SL: Kernel methods and SVMs}{chapter.2}% 31
\BOOKMARK [1][-]{section.2.8}{SL: Comp Learning Theory}{chapter.2}% 32
\BOOKMARK [1][-]{section.2.9}{SL: VC dimensions}{chapter.2}% 33
\BOOKMARK [1][-]{section.2.10}{SL: Bayesian Learning}{chapter.2}% 34
\BOOKMARK [1][-]{section.2.11}{SL: Bayesian inference}{chapter.2}% 35
\BOOKMARK [0][-]{chapter.3}{Unsupervised learning}{}% 36
\BOOKMARK [1][-]{section.3.1}{Clustering}{chapter.3}% 37
\BOOKMARK [2][-]{subsection.3.1.1}{K-means clustering}{section.3.1}% 38
\BOOKMARK [2][-]{subsection.3.1.2}{Principal Component Analysis \(PCA\)}{section.3.1}% 39
\BOOKMARK [0][-]{chapter.4}{Reinforced Learning}{}% 40
\BOOKMARK [1][-]{section.4.1}{Markov Decision Process}{chapter.4}% 41
\BOOKMARK [0][-]{chapter.5}{Miscellaneous topics}{}% 42
\BOOKMARK [1][-]{section.5.1}{One hot encoding for categorical features}{chapter.5}% 43
\BOOKMARK [1][-]{section.5.2}{Cross validation}{chapter.5}% 44
\BOOKMARK [1][-]{section.5.3}{F1 score, precision, and recall}{chapter.5}% 45
\BOOKMARK [1][-]{section.5.4}{Overfitting vs underfitting}{chapter.5}% 46
\BOOKMARK [1][-]{section.5.5}{Gradient descent \(steepest descent\)}{chapter.5}% 47
\BOOKMARK [2][-]{subsection.5.5.1}{Cost function}{section.5.5}% 48
\BOOKMARK [2][-]{subsection.5.5.2}{Gradient descent method}{section.5.5}% 49
\BOOKMARK [2][-]{subsection.5.5.3}{Learning rate selection}{section.5.5}% 50
\BOOKMARK [1][-]{section.5.6}{Model validation}{chapter.5}% 51
\BOOKMARK [1][-]{section.5.7}{Data processing/cleaning: Missing Values}{chapter.5}% 52
\BOOKMARK [1][-]{section.5.8}{Data processing \046 Feature engineering}{chapter.5}% 53
\BOOKMARK [2][-]{subsection.5.8.1}{Feature engineering}{section.5.8}% 54
\BOOKMARK [2][-]{subsection.5.8.2}{How to create new features}{section.5.8}% 55
\BOOKMARK [2][-]{subsection.5.8.3}{scaling vs normalization}{section.5.8}% 56
\BOOKMARK [2][-]{subsection.5.8.4}{Scaling a feature}{section.5.8}% 57
\BOOKMARK [2][-]{subsection.5.8.5}{Mean normalization}{section.5.8}% 58
\BOOKMARK [2][-]{subsection.5.8.6}{z-score normalization}{section.5.8}% 59
\BOOKMARK [2][-]{subsection.5.8.7}{Combining features}{section.5.8}% 60
\BOOKMARK [1][-]{section.5.9}{Ensemble methods}{chapter.5}% 61
\BOOKMARK [2][-]{subsection.5.9.1}{Gradient Boosting}{section.5.9}% 62
\BOOKMARK [2][-]{subsection.5.9.2}{Data leakage}{section.5.9}% 63
\BOOKMARK [1][-]{section.5.10}{Python}{chapter.5}% 64
\BOOKMARK [1][-]{section.5.11}{ML Checklist}{chapter.5}% 65
\BOOKMARK [2][-]{subsection.5.11.1}{Time series}{section.5.11}% 66
\BOOKMARK [1][-]{section.5.12}{Time series}{chapter.5}% 67
\BOOKMARK [2][-]{subsection.5.12.1}{Seasonality}{section.5.12}% 68
\BOOKMARK [2][-]{subsection.5.12.2}{Serial dependence}{section.5.12}% 69
\BOOKMARK [0][-]{section*.14}{Bibliography}{}% 70
