\BOOKMARK [0][-]{toc.0}{Contents}{}% 1
\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 2
\BOOKMARK [1][-]{section.1.1}{General}{chapter.1}% 3
\BOOKMARK [0][-]{chapter.2}{Supervised learning}{}% 4
\BOOKMARK [1][-]{section.2.1}{Terms}{chapter.2}% 5
\BOOKMARK [1][-]{section.2.2}{SL: Decision tree}{chapter.2}% 6
\BOOKMARK [2][-]{subsection.2.2.1}{Introduction}{section.2.2}% 7
\BOOKMARK [2][-]{subsection.2.2.2}{Representation of the decision tree:}{section.2.2}% 8
\BOOKMARK [2][-]{subsection.2.2.3}{Algorithm to build a decision tree}{section.2.2}% 9
\BOOKMARK [2][-]{subsection.2.2.4}{Expressiveness}{section.2.2}% 10
\BOOKMARK [2][-]{subsection.2.2.5}{ID3 algorithm to create a decision tree}{section.2.2}% 11
\BOOKMARK [2][-]{subsection.2.2.6}{Inductive bias}{section.2.2}% 12
\BOOKMARK [2][-]{subsection.2.2.7}{Extending ID3 algorithm-other consideration}{section.2.2}% 13
\BOOKMARK [2][-]{subsection.2.2.8}{Regression with Decision tree}{section.2.2}% 14
\BOOKMARK [2][-]{subsection.2.2.9}{Changing the information gain formula is another option}{section.2.2}% 15
\BOOKMARK [2][-]{subsection.2.2.10}{Advantages and Disadvantages of Decision Trees \(copied from a text\)}{section.2.2}% 16
\BOOKMARK [2][-]{subsection.2.2.11}{When to use a decision tree}{section.2.2}% 17
\BOOKMARK [2][-]{subsection.2.2.12}{Random forest \(Tree ensembles\)}{section.2.2}% 18
\BOOKMARK [2][-]{subsection.2.2.13}{Boosted tree \(XGBoost\)}{section.2.2}% 19
\BOOKMARK [1][-]{section.2.3}{SL: Regression and classification}{chapter.2}% 20
\BOOKMARK [2][-]{subsection.2.3.1}{Linear regression}{section.2.3}% 21
\BOOKMARK [2][-]{subsection.2.3.2}{Non-linear/polynomial regression}{section.2.3}% 22
\BOOKMARK [2][-]{subsection.2.3.3}{Error}{section.2.3}% 23
\BOOKMARK [2][-]{subsection.2.3.4}{Multiple Linear regression \(multiple features\)}{section.2.3}% 24
\BOOKMARK [2][-]{subsection.2.3.5}{Other cases}{section.2.3}% 25
\BOOKMARK [2][-]{subsection.2.3.6}{Classification}{section.2.3}% 26
\BOOKMARK [1][-]{section.2.4}{SL: Neural network}{chapter.2}% 27
\BOOKMARK [2][-]{subsection.2.4.1}{Perceptron}{section.2.4}% 28
\BOOKMARK [2][-]{subsection.2.4.2}{Logistic regression algo with Neural Network}{section.2.4}% 29
\BOOKMARK [2][-]{subsection.2.4.3}{Activation functions}{section.2.4}% 30
\BOOKMARK [2][-]{subsection.2.4.4}{Convolutional Neural Network \(CNN\)}{section.2.4}% 31
\BOOKMARK [1][-]{section.2.5}{SL: Instance based learning}{chapter.2}% 32
\BOOKMARK [1][-]{section.2.6}{SL: Ensemble B\046B}{chapter.2}% 33
\BOOKMARK [1][-]{section.2.7}{SL: Kernel methods and SVMs}{chapter.2}% 34
\BOOKMARK [1][-]{section.2.8}{SL: Comp Learning Theory}{chapter.2}% 35
\BOOKMARK [1][-]{section.2.9}{SL: VC dimensions}{chapter.2}% 36
\BOOKMARK [1][-]{section.2.10}{SL: Bayesian Learning}{chapter.2}% 37
\BOOKMARK [1][-]{section.2.11}{SL: Bayesian inference}{chapter.2}% 38
\BOOKMARK [0][-]{chapter.3}{Unsupervised learning}{}% 39
\BOOKMARK [1][-]{section.3.1}{Clustering}{chapter.3}% 40
\BOOKMARK [2][-]{subsection.3.1.1}{K-means clustering}{section.3.1}% 41
\BOOKMARK [2][-]{subsection.3.1.2}{Principal Component Analysis \(PCA\)}{section.3.1}% 42
\BOOKMARK [0][-]{chapter.4}{Reinforced Learning}{}% 43
\BOOKMARK [1][-]{section.4.1}{Markov Decision Process}{chapter.4}% 44
\BOOKMARK [0][-]{chapter.5}{Miscellaneous topics}{}% 45
\BOOKMARK [1][-]{section.5.1}{One hot encoding for categorical features}{chapter.5}% 46
\BOOKMARK [1][-]{section.5.2}{Cross validation}{chapter.5}% 47
\BOOKMARK [1][-]{section.5.3}{F1 score, precision, and recall}{chapter.5}% 48
\BOOKMARK [1][-]{section.5.4}{Optimization}{chapter.5}% 49
\BOOKMARK [2][-]{subsection.5.4.1}{Cost function}{section.5.4}% 50
\BOOKMARK [2][-]{subsection.5.4.2}{Gradient descent \(steepest descent\) method}{section.5.4}% 51
\BOOKMARK [2][-]{subsection.5.4.3}{Learning rate selection}{section.5.4}% 52
\BOOKMARK [2][-]{subsection.5.4.4}{Adam algorithm for optimization}{section.5.4}% 53
\BOOKMARK [1][-]{section.5.5}{Overfitting \(high variance\) vs underfitting \(high bias\)}{chapter.5}% 54
\BOOKMARK [1][-]{section.5.6}{Regularization}{chapter.5}% 55
\BOOKMARK [1][-]{section.5.7}{Model validation}{chapter.5}% 56
\BOOKMARK [1][-]{section.5.8}{Data processing/cleaning: Missing Values}{chapter.5}% 57
\BOOKMARK [1][-]{section.5.9}{Data processing \046 Feature engineering}{chapter.5}% 58
\BOOKMARK [2][-]{subsection.5.9.1}{Feature engineering}{section.5.9}% 59
\BOOKMARK [2][-]{subsection.5.9.2}{How to create new features}{section.5.9}% 60
\BOOKMARK [2][-]{subsection.5.9.3}{scaling vs normalization}{section.5.9}% 61
\BOOKMARK [2][-]{subsection.5.9.4}{Scaling a feature}{section.5.9}% 62
\BOOKMARK [2][-]{subsection.5.9.5}{Mean normalization}{section.5.9}% 63
\BOOKMARK [2][-]{subsection.5.9.6}{z-score normalization}{section.5.9}% 64
\BOOKMARK [2][-]{subsection.5.9.7}{Combining features}{section.5.9}% 65
\BOOKMARK [1][-]{section.5.10}{Ensemble methods}{chapter.5}% 66
\BOOKMARK [2][-]{subsection.5.10.1}{Gradient Boosting}{section.5.10}% 67
\BOOKMARK [2][-]{subsection.5.10.2}{Data leakage}{section.5.10}% 68
\BOOKMARK [1][-]{section.5.11}{Python}{chapter.5}% 69
\BOOKMARK [1][-]{section.5.12}{ML Checklist}{chapter.5}% 70
\BOOKMARK [2][-]{subsection.5.12.1}{Time series}{section.5.12}% 71
\BOOKMARK [1][-]{section.5.13}{Time series}{chapter.5}% 72
\BOOKMARK [2][-]{subsection.5.13.1}{Trend}{section.5.13}% 73
\BOOKMARK [2][-]{subsection.5.13.2}{Seasonality}{section.5.13}% 74
\BOOKMARK [2][-]{subsection.5.13.3}{Cycles: serial dependence \(time series as features\)}{section.5.13}% 75
\BOOKMARK [2][-]{subsection.5.13.4}{Forecasting with hybrid models: Components and residuals}{section.5.13}% 76
\BOOKMARK [2][-]{subsection.5.13.5}{Forecasting}{section.5.13}% 77
\BOOKMARK [0][-]{figure.5.11}{Bibliography}{}% 78
