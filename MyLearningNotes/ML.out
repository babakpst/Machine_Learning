\BOOKMARK [0][-]{toc.0}{Contents}{}% 1
\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 2
\BOOKMARK [1][-]{section.1.1}{General}{chapter.1}% 3
\BOOKMARK [0][-]{chapter.2}{Supervised learning}{}% 4
\BOOKMARK [1][-]{section.2.1}{Terms}{chapter.2}% 5
\BOOKMARK [1][-]{section.2.2}{SL: Decision tree}{chapter.2}% 6
\BOOKMARK [2][-]{subsection.2.2.1}{Introduction}{section.2.2}% 7
\BOOKMARK [2][-]{subsection.2.2.2}{Representation of the decision tree:}{section.2.2}% 8
\BOOKMARK [2][-]{subsection.2.2.3}{Algorithm to build a decision tree}{section.2.2}% 9
\BOOKMARK [2][-]{subsection.2.2.4}{Expressiveness}{section.2.2}% 10
\BOOKMARK [2][-]{subsection.2.2.5}{ID3 algorithm to create a decision tree}{section.2.2}% 11
\BOOKMARK [2][-]{subsection.2.2.6}{Inductive bias}{section.2.2}% 12
\BOOKMARK [2][-]{subsection.2.2.7}{Extending ID3 algorithm-other consideration}{section.2.2}% 13
\BOOKMARK [2][-]{subsection.2.2.8}{Regression with Decision tree}{section.2.2}% 14
\BOOKMARK [2][-]{subsection.2.2.9}{Changing the information gain formula is another option}{section.2.2}% 15
\BOOKMARK [2][-]{subsection.2.2.10}{Advantages and Disadvantages of Decision Trees \(copied from a text\)}{section.2.2}% 16
\BOOKMARK [2][-]{subsection.2.2.11}{When to use a decision tree}{section.2.2}% 17
\BOOKMARK [2][-]{subsection.2.2.12}{Random forest \(Tree ensembles\)}{section.2.2}% 18
\BOOKMARK [2][-]{subsection.2.2.13}{Boosted tree \(XGBoost\)}{section.2.2}% 19
\BOOKMARK [1][-]{section.2.3}{SL: Regression and classification}{chapter.2}% 20
\BOOKMARK [2][-]{subsection.2.3.1}{Linear regression}{section.2.3}% 21
\BOOKMARK [2][-]{subsection.2.3.2}{Non-linear/polynomial regression}{section.2.3}% 22
\BOOKMARK [2][-]{subsection.2.3.3}{Error}{section.2.3}% 23
\BOOKMARK [2][-]{subsection.2.3.4}{Multiple Linear regression \(multiple features\)}{section.2.3}% 24
\BOOKMARK [2][-]{subsection.2.3.5}{Other cases}{section.2.3}% 25
\BOOKMARK [2][-]{subsection.2.3.6}{Classification}{section.2.3}% 26
\BOOKMARK [1][-]{section.2.4}{SL: Neural network}{chapter.2}% 27
\BOOKMARK [2][-]{subsection.2.4.1}{Perceptron}{section.2.4}% 28
\BOOKMARK [2][-]{subsection.2.4.2}{Logistic regression algo with Neural Network}{section.2.4}% 29
\BOOKMARK [2][-]{subsection.2.4.3}{Activation functions}{section.2.4}% 30
\BOOKMARK [2][-]{subsection.2.4.4}{Biases}{section.2.4}% 31
\BOOKMARK [2][-]{subsection.2.4.5}{Convolutional Neural Network \(CNN\)}{section.2.4}% 32
\BOOKMARK [2][-]{subsection.2.4.6}{Tuning hyperparameters}{section.2.4}% 33
\BOOKMARK [2][-]{subsection.2.4.7}{Error analysis}{section.2.4}% 34
\BOOKMARK [1][-]{section.2.5}{SL: Sequence models-Recurrent Neural Network \(RNN\)}{chapter.2}% 35
\BOOKMARK [1][-]{section.2.6}{SL: Instance based learning}{chapter.2}% 36
\BOOKMARK [1][-]{section.2.7}{SL: Ensemble B\046B}{chapter.2}% 37
\BOOKMARK [1][-]{section.2.8}{SL: Kernel methods and SVMs}{chapter.2}% 38
\BOOKMARK [1][-]{section.2.9}{SL: Comp Learning Theory}{chapter.2}% 39
\BOOKMARK [1][-]{section.2.10}{SL: VC dimensions}{chapter.2}% 40
\BOOKMARK [1][-]{section.2.11}{SL: Bayesian Learning}{chapter.2}% 41
\BOOKMARK [1][-]{section.2.12}{SL: Bayesian inference}{chapter.2}% 42
\BOOKMARK [0][-]{chapter.3}{Unsupervised learning}{}% 43
\BOOKMARK [1][-]{section.3.1}{Clustering}{chapter.3}% 44
\BOOKMARK [2][-]{subsection.3.1.1}{K-means clustering}{section.3.1}% 45
\BOOKMARK [2][-]{subsection.3.1.2}{Principal Component Analysis \(PCA\)}{section.3.1}% 46
\BOOKMARK [1][-]{section.3.2}{Anomaly detection}{chapter.3}% 47
\BOOKMARK [0][-]{chapter.4}{Reinforced Learning}{}% 48
\BOOKMARK [1][-]{section.4.1}{Markov Decision Process}{chapter.4}% 49
\BOOKMARK [0][-]{chapter.5}{Large Language Models \(LLM\) and Transformers}{}% 50
\BOOKMARK [1][-]{section.5.1}{Introduction}{chapter.5}% 51
\BOOKMARK [1][-]{section.5.2}{Transformer types}{chapter.5}% 52
\BOOKMARK [1][-]{section.5.3}{Prompt engineering}{chapter.5}% 53
\BOOKMARK [1][-]{section.5.4}{Project life cycle}{chapter.5}% 54
\BOOKMARK [1][-]{section.5.5}{Quantization}{chapter.5}% 55
\BOOKMARK [1][-]{section.5.6}{Scale computation}{chapter.5}% 56
\BOOKMARK [0][-]{chapter.6}{Miscellaneous topics}{}% 57
\BOOKMARK [1][-]{section.6.1}{One hot encoding for categorical features}{chapter.6}% 58
\BOOKMARK [1][-]{section.6.2}{Evaluating the model-partitioning the data set}{chapter.6}% 59
\BOOKMARK [1][-]{section.6.3}{Cross validation method}{chapter.6}% 60
\BOOKMARK [1][-]{section.6.4}{Accuracy of the model: F1 score, precision, and recall}{chapter.6}% 61
\BOOKMARK [1][-]{section.6.5}{Optimization}{chapter.6}% 62
\BOOKMARK [2][-]{subsection.6.5.1}{Cost function}{section.6.5}% 63
\BOOKMARK [2][-]{subsection.6.5.2}{Gradient descent \(steepest descent\) method}{section.6.5}% 64
\BOOKMARK [2][-]{subsection.6.5.3}{Learning rate selection}{section.6.5}% 65
\BOOKMARK [1][-]{section.6.6}{Overfitting \(high variance\) vs underfitting \(high bias\)}{chapter.6}% 66
\BOOKMARK [1][-]{section.6.7}{Transfer learning and multi-task learning}{chapter.6}% 67
\BOOKMARK [1][-]{section.6.8}{Regularization}{chapter.6}% 68
\BOOKMARK [1][-]{section.6.9}{Model validation}{chapter.6}% 69
\BOOKMARK [1][-]{section.6.10}{Data processing/cleaning: Missing Values}{chapter.6}% 70
\BOOKMARK [1][-]{section.6.11}{Data processing \046 Feature engineering}{chapter.6}% 71
\BOOKMARK [2][-]{subsection.6.11.1}{Feature engineering}{section.6.11}% 72
\BOOKMARK [2][-]{subsection.6.11.2}{How to create new features}{section.6.11}% 73
\BOOKMARK [2][-]{subsection.6.11.3}{Scaling vs normalization}{section.6.11}% 74
\BOOKMARK [2][-]{subsection.6.11.4}{Scaling a feature}{section.6.11}% 75
\BOOKMARK [2][-]{subsection.6.11.5}{Mean normalization}{section.6.11}% 76
\BOOKMARK [2][-]{subsection.6.11.6}{z-score normalization}{section.6.11}% 77
\BOOKMARK [2][-]{subsection.6.11.7}{Combining features}{section.6.11}% 78
\BOOKMARK [2][-]{subsection.6.11.8}{Batch normalization}{section.6.11}% 79
\BOOKMARK [1][-]{section.6.12}{Ensemble methods}{chapter.6}% 80
\BOOKMARK [2][-]{subsection.6.12.1}{Gradient Boosting}{section.6.12}% 81
\BOOKMARK [2][-]{subsection.6.12.2}{Data leakage}{section.6.12}% 82
\BOOKMARK [1][-]{section.6.13}{Python}{chapter.6}% 83
\BOOKMARK [1][-]{section.6.14}{ML Checklist}{chapter.6}% 84
\BOOKMARK [2][-]{subsection.6.14.1}{Time series}{section.6.14}% 85
\BOOKMARK [1][-]{section.6.15}{Time series}{chapter.6}% 86
\BOOKMARK [2][-]{subsection.6.15.1}{Trend}{section.6.15}% 87
\BOOKMARK [2][-]{subsection.6.15.2}{Seasonality}{section.6.15}% 88
\BOOKMARK [2][-]{subsection.6.15.3}{Cycles: serial dependence \(time series as features\)}{section.6.15}% 89
\BOOKMARK [2][-]{subsection.6.15.4}{Forecasting with hybrid models: Components and residuals}{section.6.15}% 90
\BOOKMARK [2][-]{subsection.6.15.5}{Forecasting}{section.6.15}% 91
\BOOKMARK [1][-]{section.6.16}{ML explainability}{chapter.6}% 92
\BOOKMARK [2][-]{subsection.6.16.1}{Feature importance}{section.6.16}% 93
\BOOKMARK [2][-]{subsection.6.16.2}{Partial dependence plots}{section.6.16}% 94
\BOOKMARK [2][-]{subsection.6.16.3}{SHAP values}{section.6.16}% 95
\BOOKMARK [0][-]{subsection.6.16.3}{Bibliography}{}% 96
