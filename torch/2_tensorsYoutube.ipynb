{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a tensor & its datatype\n",
    "\n",
    "Convert the datatype using .to()\n",
    "\n",
    "torch.bool\n",
    "\n",
    "torch.int8\n",
    "\n",
    "torch.uint8\n",
    "\n",
    "torch.int16\n",
    "\n",
    "torch.int32\n",
    "\n",
    "torch.int64\n",
    "\n",
    "torch.half\n",
    "\n",
    "torch.float\n",
    "\n",
    "torch.double\n",
    "\n",
    "torch.bfloat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[1.6629e-35, 0.0000e+00, 1.5408e-35],\n",
      "        [0.0000e+00, 1.0000e+00, 1.0000e+00]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009]])\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float16)\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#random-looking values when printing your tensor. The torch.empty() call allocates memory for the tensor, but does not initialize it with any values - so what you’re seeing is whatever was in memory at the time of allocation.\n",
    "x = torch.empty(2,3) # default datatype is float32\n",
    "print(type(x))\n",
    "print(x)\n",
    "\n",
    "zeros = torch.zeros(2,3) # default datatype is float32\n",
    "print(zeros)\n",
    "\n",
    "ones = torch.ones(2,3) # default datatype is float32\n",
    "print(ones)\n",
    "\n",
    "torch.manual_seed(42) # assurance of the reproducibility of your results.\n",
    "random = torch.rand(2,3)\n",
    "print(random)\n",
    "\n",
    "tenInt16 = torch.ones(2,3, dtype=torch.int16)\n",
    "print(tenInt16)\n",
    "\n",
    "tenF16 = torch.ones((2,3), dtype=torch.float16)\n",
    "print(tenF16)\n",
    "\n",
    "tenF64 = tenF16.double()\n",
    "print(tenF64)\n",
    " \n",
    "tenF64_2 = tenF16.to(torch.float64)\n",
    "print(tenF64_2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8261e-35, 0.0000e+00, 3.3739e+13],\n",
      "        [4.5629e-41, 8.9683e-44, 0.0000e+00]])\n",
      "torch.Size([2, 3])\n",
      "tensor([[1.5165e-35, 0.0000e+00, 1.6996e-35],\n",
      "        [0.0000e+00, 8.9683e-44, 0.0000e+00]])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.Size([2, 3])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0.2566, 0.7936, 0.9408],\n",
      "        [0.1332, 0.9346, 0.5936]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(2,3)\n",
    "print(x)\n",
    "\n",
    "empty_tensor_like_x = torch.empty_like(x)\n",
    "print(empty_tensor_like_x.shape)\n",
    "print(empty_tensor_like_x)\n",
    "\n",
    "zeros_like_x = torch.zeros_like(x)\n",
    "print(zeros_like_x.shape)\n",
    "print(zeros_like_x)\n",
    "\n",
    "onees_like_x = torch.ones_like(x)\n",
    "print(onees_like_x.shape)\n",
    "print(onees_like_x)\n",
    "\n",
    "rand_like_x = torch.rand_like(x)\n",
    "print(rand_like_x.shape)\n",
    "print(rand_like_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a tensor by its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([5, 6, 7, 8])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "ten1 = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(ten1)\n",
    "\n",
    "ten2 = torch.tensor((5,6,7,8))\n",
    "print(ten2)\n",
    "\n",
    "ten3 = torch.tensor(((1,2,3),[4,5,6]))\n",
    "print(ten3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n",
      "tensor([[1.4142, 1.4142],\n",
      "        [1.4142, 1.4142]])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.zeros(2, 2) + 1\n",
    "twos = torch.ones(2, 2) * 2\n",
    "threes = (torch.ones(2, 2) * 7 - 1) / 2\n",
    "fours = twos ** 2\n",
    "sqrt2s = twos ** 0.5\n",
    "\n",
    "print(ones)\n",
    "print(twos)\n",
    "print(threes)\n",
    "print(fours)\n",
    "print(sqrt2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.],\n",
      "        [ 8., 16.]])\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.]])\n",
      "tensor([[12., 12.],\n",
      "        [12., 12.]])\n"
     ]
    }
   ],
   "source": [
    "powers2 = twos ** torch.tensor([[1, 2], [3, 4]])\n",
    "print(powers2)\n",
    "\n",
    "fives = ones + fours\n",
    "print(fives)\n",
    "\n",
    "dozens = threes * fours # this is not a matmul\n",
    "print(dozens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor broadcast\n",
    "\n",
    "How is it we got to multiply a 2x4 tensor by a 1x4 tensor?\n",
    "\n",
    "Broadcasting is a way to perform an operation between tensors that have similarities in their shapes. In the example above, the one-row, four-column tensor is multiplied by both rows of the two-row, four-column tensor.\n",
    "\n",
    "This is an important operation in Deep Learning. The common example is multiplying a tensor of learning weights by a batch of input tensors, applying the operation to each instance in the batch separately, and returning a tensor of identical shape - just like our (2, 4) * (1, 4) example above returned a tensor of shape (2, 4).\n",
    "\n",
    "The rules for broadcasting are:\n",
    "\n",
    "Each tensor must have at least one dimension - no empty tensors.\n",
    "\n",
    "Comparing the dimension sizes of the two tensors, going from last to first:\n",
    "\n",
    "Each dimension must be equal, or\n",
    "\n",
    "One of the dimensions must be of size 1, or\n",
    "\n",
    "The dimension does not exist in one of the tensors\n",
    "\n",
    "Tensors of identical shape, of course, are trivially “broadcastable”, as you saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6666, 0.9811, 0.0874, 0.0041],\n",
      "        [0.1088, 0.1637, 0.7025, 0.6790]])\n",
      "tensor([[1.3333, 1.9623, 0.1747, 0.0081],\n",
      "        [0.2176, 0.3273, 1.4050, 1.3581]])\n",
      "tensor([[0.9155, 0.2418],\n",
      "        [0.1591, 0.7653],\n",
      "        [0.2979, 0.8035]])\n",
      "tensor([[[0.9155, 0.2418],\n",
      "         [0.1591, 0.7653],\n",
      "         [0.2979, 0.8035]],\n",
      "\n",
      "        [[0.9155, 0.2418],\n",
      "         [0.1591, 0.7653],\n",
      "         [0.2979, 0.8035]],\n",
      "\n",
      "        [[0.9155, 0.2418],\n",
      "         [0.1591, 0.7653],\n",
      "         [0.2979, 0.8035]],\n",
      "\n",
      "        [[0.9155, 0.2418],\n",
      "         [0.1591, 0.7653],\n",
      "         [0.2979, 0.8035]]])\n",
      "tensor([[0.3813],\n",
      "        [0.7860],\n",
      "        [0.1115]])\n",
      "tensor([[[0.3813, 0.3813],\n",
      "         [0.7860, 0.7860],\n",
      "         [0.1115, 0.1115]],\n",
      "\n",
      "        [[0.3813, 0.3813],\n",
      "         [0.7860, 0.7860],\n",
      "         [0.1115, 0.1115]],\n",
      "\n",
      "        [[0.3813, 0.3813],\n",
      "         [0.7860, 0.7860],\n",
      "         [0.1115, 0.1115]],\n",
      "\n",
      "        [[0.3813, 0.3813],\n",
      "         [0.7860, 0.7860],\n",
      "         [0.1115, 0.1115]]])\n",
      "tensor([[[0.2477, 0.6524],\n",
      "         [0.2477, 0.6524],\n",
      "         [0.2477, 0.6524]],\n",
      "\n",
      "        [[0.2477, 0.6524],\n",
      "         [0.2477, 0.6524],\n",
      "         [0.2477, 0.6524]],\n",
      "\n",
      "        [[0.2477, 0.6524],\n",
      "         [0.2477, 0.6524],\n",
      "         [0.2477, 0.6524]],\n",
      "\n",
      "        [[0.2477, 0.6524],\n",
      "         [0.2477, 0.6524],\n",
      "         [0.2477, 0.6524]]])\n"
     ]
    }
   ],
   "source": [
    "rand = torch.rand(2, 4)\n",
    "doubled = rand * (torch.ones(1, 4) * 2)\n",
    "\n",
    "print(rand)\n",
    "print(doubled)\n",
    "\n",
    "a =     torch.ones(4, 3, 2)\n",
    "mm = torch.rand(   3, 2) # 3rd & 2nd dims identical to a, dim 1 absent\n",
    "print(mm)\n",
    "b = a * mm\n",
    "print(b)\n",
    "\n",
    "mm = torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a\n",
    "print(mm)\n",
    "c = a * mm\n",
    "print(c)\n",
    "\n",
    "d = a * torch.rand(   1, 2) # 3rd dim identical to a, 2nd dim = 1\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following cell throws a run-time error. This is intentional.\n",
    "\n",
    "a =     torch.ones(4, 3, 2)\n",
    "\n",
    "#b = a * torch.rand(4, 3)    # dimensions must match last-to-first\n",
    "#c = a * torch.rand(   2, 3) # both 3rd & 2nd dims different\n",
    "#d = a * torch.rand((0, ))   # can't broadcast with an empty tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math operation\n",
    "\n",
    "more operations from torch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common functions:\n",
      "tensor([[0.1320, 0.7259, 0.0235, 0.6831],\n",
      "        [0.8484, 0.5507, 0.8752, 0.6367]])\n",
      "tensor([[-0., -0., 1., -0.],\n",
      "        [-0., -0., -0., -0.]])\n",
      "tensor([[-1., -1.,  0., -1.],\n",
      "        [-1., -1., -1., -1.]])\n",
      "tensor([[-0.1320, -0.5000,  0.0235, -0.5000],\n",
      "        [-0.5000, -0.5000, -0.5000, -0.5000]])\n",
      "\n",
      "Sine and arcsine:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 0.7854])\n",
      "\n",
      "Bitwise XOR:\n",
      "tensor([3, 2, 1])\n",
      "\n",
      "Broadcasted, element-wise equality comparison:\n",
      "tensor([[ True, False],\n",
      "        [False, False]])\n",
      "\n",
      "Reduction ops:\n",
      "tensor(4.)\n",
      "4.0\n",
      "tensor(2.5000)\n",
      "tensor(1.2910)\n",
      "tensor(24.)\n",
      "tensor([1, 2])\n",
      "\n",
      "Vectors & Matrices:\n",
      "tensor([ 0.,  0., -1.])\n",
      "tensor([[0.9998, 0.5944],\n",
      "        [0.6541, 0.0337]])\n",
      "tensor([[2.9994, 1.7833],\n",
      "        [1.9622, 0.1010]])\n",
      "torch.return_types.svd(\n",
      "U=tensor([[-0.8847, -0.4661],\n",
      "        [-0.4661,  0.8847]]),\n",
      "S=tensor([3.9208, 0.8152]),\n",
      "V=tensor([[-0.9101,  0.4144],\n",
      "        [-0.4144, -0.9101]]))\n"
     ]
    }
   ],
   "source": [
    "# common functions\n",
    "a = torch.rand(2, 4) * 2 - 1\n",
    "print('Common functions:')\n",
    "print(torch.abs(a))\n",
    "print(torch.ceil(a))\n",
    "print(torch.floor(a))\n",
    "print(torch.clamp(a, -0.5, 0.5))\n",
    "\n",
    "# trigonometric functions and their inverses\n",
    "angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "sines = torch.sin(angles)\n",
    "inverses = torch.asin(sines)\n",
    "print('\\nSine and arcsine:')\n",
    "print(angles)\n",
    "print(sines)\n",
    "print(inverses)\n",
    "\n",
    "# bitwise operations\n",
    "print('\\nBitwise XOR:')\n",
    "b = torch.tensor([1, 5, 11])\n",
    "c = torch.tensor([2, 7, 10])\n",
    "print(torch.bitwise_xor(b, c))\n",
    "\n",
    "# comparisons:\n",
    "print('\\nBroadcasted, element-wise equality comparison:')\n",
    "d = torch.tensor([[1., 2.], [3., 4.]])\n",
    "e = torch.ones(1, 2)  # many comparison ops support broadcasting!\n",
    "print(torch.eq(d, e)) # returns a tensor of type bool\n",
    "\n",
    "# reductions:\n",
    "print('\\nReduction ops:')\n",
    "print(torch.max(d))        # returns a single-element tensor\n",
    "print(torch.max(d).item()) # extracts the value from the returned tensor\n",
    "print(torch.mean(d))       # average\n",
    "print(torch.std(d))        # standard deviation\n",
    "print(torch.prod(d))       # product of all numbers\n",
    "print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # filter unique elements\n",
    "\n",
    "# vector and linear algebra operations\n",
    "v1 = torch.tensor([1., 0., 0.])         # x unit vector\n",
    "v2 = torch.tensor([0., 1., 0.])         # y unit vector\n",
    "m1 = torch.rand(2, 2)                   # random matrix\n",
    "m2 = torch.tensor([[3., 0.], [0., 3.]]) # three times identity matrix\n",
    "\n",
    "print('\\nVectors & Matrices:')\n",
    "print(torch.cross(v2, v1)) # negative of z unit vector (v1 x v2 == -v2 x v1)\n",
    "print(m1)\n",
    "m3 = torch.matmul(m1, m2)\n",
    "print(m3)                  # 3 times m1\n",
    "print(torch.svd(m3))       # singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in-place math operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "\n",
      "b:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "\n",
      "\n",
      " ---Before:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.4654, 0.1612],\n",
      "        [0.1568, 0.2083]])\n",
      "\n",
      "After adding:\n",
      "tensor([[1.4654, 1.1612],\n",
      "        [1.1568, 1.2083]])\n",
      "tensor([[1.4654, 1.1612],\n",
      "        [1.1568, 1.2083]])\n",
      "tensor([[0.4654, 0.1612],\n",
      "        [0.1568, 0.2083]])\n",
      "\n",
      "After multiplying\n",
      "tensor([[0.2166, 0.0260],\n",
      "        [0.0246, 0.0434]])\n",
      "tensor([[0.2166, 0.0260],\n",
      "        [0.0246, 0.0434]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('a:')\n",
    "print(a)\n",
    "print(torch.sin(a))   # this operation creates a new tensor in memory\n",
    "print(a)              # a has not changed\n",
    "\n",
    "b = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('\\nb:')\n",
    "print(b)\n",
    "print(torch.sin_(b))  # note the underscore\n",
    "print(b)              # b has changed\n",
    "\n",
    "a = torch.ones(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "\n",
    "print('\\n\\n ---Before:')\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter adding:')\n",
    "print(a.add_(b))\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter multiplying')\n",
    "print(b.mul_(b))\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another option for placing the result of a computation in an existing, allocated tensor. Many of the methods and functions we’ve seen so far - including creation methods! - have an out argument that lets you specify a tensor to receive the output. If the out tensor is the correct shape and dtype, this can happen without a new memory allocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139849393002112\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0.8166, 0.2758],\n",
      "        [0.5218, 0.1442]])\n",
      "tensor([[0.6625, 0.2297],\n",
      "        [0.9545, 0.6099]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "c = torch.zeros(2, 2)\n",
    "old_id = id(c) # object's memory address\n",
    "\n",
    "print(old_id)\n",
    "\n",
    "print(c)\n",
    "d = torch.matmul(a, b, out=c)\n",
    "print(c)                # contents of c have changed\n",
    "\n",
    "assert c is d           # test c & d are same object, not just containing equal values\n",
    "assert id(c) == old_id  # make sure that our new c is the same object as the old one\n",
    "\n",
    "torch.rand(2, 2, out=c) # works for creation too!\n",
    "print(c)                # c has changed again\n",
    "assert id(c) == old_id  # still the same object!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copying tensors\n",
    "Assigning a tensor to a variable makes the variable a label of the tensor, and does not copy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1., 561.],\n",
      "        [  1.,   1.]])\n",
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a # just a reference to a, not a copy\n",
    "\n",
    "a[0][1] = 561  # we change a...\n",
    "print(b)       # ...and b is also altered\n",
    "\n",
    "\n",
    "a = torch.ones(2, 2)\n",
    "b = a.clone() # make a copy\n",
    "\n",
    "assert b is not a      # different objects in memory...\n",
    "print(torch.eq(a, b))  # ...but still with the same contents!\n",
    "\n",
    "a[0][1] = 561          # a changes...\n",
    "print(b)               # ...but b is still all ones\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to detach autograd from the tensor, use the detach to just copy the data for more efficient data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5643, 0.0594],\n",
      "        [0.7099, 0.4250]], requires_grad=True)\n",
      "tensor([[0.5643, 0.0594],\n",
      "        [0.7099, 0.4250]], grad_fn=<CloneBackward0>)\n",
      "tensor([[0.5643, 0.0594],\n",
      "        [0.7099, 0.4250]])\n",
      "tensor([[0.5643, 0.0594],\n",
      "        [0.7099, 0.4250]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2, requires_grad=True) # turn on autograd\n",
    "print(a)\n",
    "\n",
    "b = a.clone()\n",
    "print(b)\n",
    "\n",
    "c = a.detach().clone()\n",
    "print(c) # no gradient tracking\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a GPU!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('We have a GPU!')\n",
    "else:\n",
    "    print('Sorry, CPU only.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6130, 0.0101],\n",
      "        [0.3984, 0.0403]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_rand = torch.rand(2, 2, device='cuda')\n",
    "    print(gpu_rand)\n",
    "else:\n",
    "    print('Sorry, CPU only.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to develop a code that is robust (regardless of the CPU/GPU), we can create a handle, and define tesnsors based on the GPU. \n",
    "\n",
    "If you have an existing tensor living on one device, you can move it to another with the to() method. The following line of code creates a tensor on CPU, and moves it to whichever device handle you acquired in the previous cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "tensor([[0.9877, 0.1289],\n",
      "        [0.5621, 0.5221]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device('cuda')\n",
    "else:\n",
    "    my_device = torch.device('cpu')\n",
    "print('Device: {}'.format(my_device))\n",
    "\n",
    "x = torch.rand(2, 2, device=my_device)\n",
    "print(x)\n",
    "\n",
    "\n",
    "y = torch.rand(2, 2)\n",
    "y = y.to(my_device) # transfer to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to do computation involving two or more tensors, all of the tensors must be on the same device. The following code will throw a runtime error, regardless of whether you have a GPU device available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2, 2, device='cuda')\n",
    "#z = x + y  # exception will be thrown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating Tensor Shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " imagine having a model that works on 3 x 226 x 226 images - a 226-pixel square with 3 color channels. When you load and transform it, you’ll get a tensor of shape (3, 226, 226). Your model, though, is expecting input of shape (N, 3, 226, 226), where N is the number of images in the batch. So how do you make a batch of one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n",
      "torch.Size([3, 1, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 226, 226)\n",
    "b = a.unsqueeze(0) # The unsqueeze() method adds a dimension of extent 1. unsqueeze(0) adds it as a new zeroth dimension - now you have a batch of one!\n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "\n",
    "c = a.unsqueeze(1)\n",
    "print(c.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you’ll see that printing a shows an “extra” set of square brackets [] due to having an extra dimension.\n",
    "\n",
    "You may only squeeze() dimensions of extent 1. See above where we try to squeeze a dimension of size 2 in c, and get back the same shape we started with. Calls to squeeze() and unsqueeze() can only act on dimensions of extent 1 because to do otherwise would change the number of elements in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[0.7197]]]]])\n",
      "torch.Size([1, 20])\n",
      "tensor([[0.8354, 0.4693, 0.8818, 0.8335, 0.2370, 0.9629, 0.3358, 0.8241, 0.1101,\n",
      "         0.8760, 0.7264, 0.4484, 0.7953, 0.4162, 0.7104, 0.5623, 0.3949, 0.9325,\n",
      "         0.9623, 0.3244]])\n",
      "torch.Size([20])\n",
      "tensor([0.8354, 0.4693, 0.8818, 0.8335, 0.2370, 0.9629, 0.3358, 0.8241, 0.1101,\n",
      "        0.8760, 0.7264, 0.4484, 0.7953, 0.4162, 0.7104, 0.5623, 0.3949, 0.9325,\n",
      "        0.9623, 0.3244])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "c = torch.rand(1, 1, 1, 1, 1)\n",
    "print(c)\n",
    "\n",
    "a = torch.rand(1, 20)\n",
    "print(a.shape)\n",
    "print(a)\n",
    "\n",
    "b = a.squeeze(0) # removes the first dimension, \n",
    "print(b.shape)\n",
    "print(b) # see the difference in the shape and contents of a and b\n",
    "\n",
    "c = torch.rand(2, 2)\n",
    "print(c.shape)\n",
    "\n",
    "d = c.squeeze(0)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3232, 0.3232],\n",
      "         [0.5999, 0.5999],\n",
      "         [0.0262, 0.0262]],\n",
      "\n",
      "        [[0.3232, 0.3232],\n",
      "         [0.5999, 0.5999],\n",
      "         [0.0262, 0.0262]],\n",
      "\n",
      "        [[0.3232, 0.3232],\n",
      "         [0.5999, 0.5999],\n",
      "         [0.0262, 0.0262]],\n",
      "\n",
      "        [[0.3232, 0.3232],\n",
      "         [0.5999, 0.5999],\n",
      "         [0.0262, 0.0262]]])\n",
      "torch.Size([3, 1])\n",
      "tensor([[[0.9205, 0.9205],\n",
      "         [0.9551, 0.9551],\n",
      "         [0.4157, 0.4157]],\n",
      "\n",
      "        [[0.9205, 0.9205],\n",
      "         [0.9551, 0.9551],\n",
      "         [0.4157, 0.4157]],\n",
      "\n",
      "        [[0.9205, 0.9205],\n",
      "         [0.9551, 0.9551],\n",
      "         [0.4157, 0.4157]],\n",
      "\n",
      "        [[0.9205, 0.9205],\n",
      "         [0.9551, 0.9551],\n",
      "         [0.4157, 0.4157]]])\n",
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n",
      "torch.Size([6, 20, 20])\n",
      "torch.Size([2400])\n",
      "torch.Size([2400])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4, 3, 2)\n",
    "\n",
    "c = a * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a\n",
    "print(c)\n",
    "\n",
    "a = torch.ones(4, 3, 2)\n",
    "b = torch.rand(   3)     # trying to multiply a * b will give a runtime error\n",
    "c = b.unsqueeze(1)       # change to a 2-dimensional tensor, adding new dim at the end\n",
    "print(c.shape)\n",
    "print(a * c)             # broadcasting works again!\n",
    "\n",
    "batch_me = torch.rand(3, 226, 226)\n",
    "print(batch_me.shape)\n",
    "batch_me.unsqueeze_(0)\n",
    "print(batch_me.shape)\n",
    "\n",
    "output3d = torch.rand(6, 20, 20)\n",
    "print(output3d.shape)\n",
    "\n",
    "input1d = output3d.reshape(6 * 20 * 20)\n",
    "print(input1d.shape)\n",
    "\n",
    "# can also call it as a method on the torch module:\n",
    "print(torch.reshape(output3d, (6 * 20 * 20,)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy and pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[0.6468, 0.4002, 0.9548],\n",
      "        [0.9986, 0.0734, 0.9361]])\n",
      "[[0.6468483  0.40015334 0.9547675 ]\n",
      " [0.9986462  0.07335663 0.93613154]]\n",
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1., 23.,  1.]], dtype=torch.float64)\n",
      "[[ 0.6468483   0.40015334  0.9547675 ]\n",
      " [ 0.9986462  17.          0.93613154]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numpy_array = np.ones((2, 3))\n",
    "print(numpy_array)\n",
    "\n",
    "pytorch_tensor = torch.from_numpy(numpy_array)\n",
    "print(pytorch_tensor)\n",
    "\n",
    "pytorch_rand = torch.rand(2, 3)\n",
    "print(pytorch_rand)\n",
    "\n",
    "numpy_rand = pytorch_rand.numpy()\n",
    "print(numpy_rand)\n",
    "\n",
    "numpy_array[1, 1] = 23\n",
    "print(pytorch_tensor)\n",
    "\n",
    "pytorch_rand[1, 1] = 17\n",
    "print(numpy_rand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
