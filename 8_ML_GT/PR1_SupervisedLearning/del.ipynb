{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 7641 HW1 Code - Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file will provide analysis for 5 different classification algorithms for two datasets.\n",
    "\n",
    "Datasets: Phishing Websites, Bank Marketing.\n",
    "\n",
    "Classification Algorithms: Decision Tree, Neural Network, Boosting, SVM, KNN.\n",
    "\n",
    "There will be two outputs for each model: a plot of learning curve and a plot of model complexity. Both plots will be generated after hyperparameter tuning is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Load and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the data! Please save the datasets to your local machine and change the current directory to a file where you have the data stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "os.chdir(r\"C:\\...\") #change this to your current working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Phishing Website Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Download the Phishing Data from OpenML https://www.openml.org/d/4534\n",
    "\n",
    "df_phish = pd.read_csv('PhishingWebsitesData.csv').astype('category')\n",
    "print(\"Data has\",len(df_phish),\"rows and\", len(df_phish.columns),\"columns.\")\n",
    "if df_phish.isnull().values.any():\n",
    "    print(\"Warning: Missing Data\")\n",
    "#df_phish.head()\n",
    "#df_phish.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the phishing data is loaded, we need to do some preprocessing. Several of the columns are categorical with the levels {-1,0,1} and the rest are all binary with levels {-1,1}. For the 3-level columns we will use one-hot encoding to create additional features with level {0,1}. Finally, we will edit the binary features so that the new levels are all {0,1}. We will have more features now, but they will all be binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_1hot = ['URL_Length','having_Sub_Domain','SSLfinal_State','URL_of_Anchor','Links_in_tags','SFH','web_traffic','Links_pointing_to_page']\n",
    "df_1hot = df_phish[col_1hot]\n",
    "df_1hot = pd.get_dummies(df_1hot)\n",
    "df_others = df_phish.drop(col_1hot,axis=1)\n",
    "df_phish = pd.concat([df_1hot,df_others],axis=1)\n",
    "df_phish = df_phish.replace(-1,0).astype('category')\n",
    "column_order = list(df_phish)\n",
    "column_order.insert(0, column_order.pop(column_order.index('Result')))\n",
    "df_phish = df_phish.loc[:, column_order]  #move the target variable 'Result' to the front\n",
    "df_phish.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a file with no missing data in the format [y, X] where all features are binary {0,1}. The phishing data is ready to go! Now we move on to loading the Bank Marketing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Bank Marketing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load the Bank Marketing Data from OpenML https://www.openml.org/d/1461\n",
    "\n",
    "df_bank = pd.read_csv('BankMarketingData.csv')\n",
    "print(\"Data has\",len(df_bank),\"rows and\", len(df_bank.columns),\"columns.\")\n",
    "if df_bank.isnull().values.any():\n",
    "    print(\"Warning: Missing Data\")\n",
    "#df_bank.head()\n",
    "#df_bank.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset needs some preprocessing love too. We will convert all categorical columns using one hot encoding. Additionally, we will standardize all of the numeric features and we will convert the target variable from {no,yes} to {0,1}. It should be noted that the feature 'pdays' is numeric but contains values that are '999' if the customer was not called before. It may be worth while to create a new feature that defines whether or not {0,1} a customer had been called before. In the current state the '999' values may be outliers. Finally we will standardize all numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_1hot = ['job','marital','education','default','housing','loan','contact','month','day_of_week','poutcome']\n",
    "df_1hot = df_bank[col_1hot]\n",
    "df_1hot = pd.get_dummies(df_1hot).astype('category')\n",
    "df_others = df_bank.drop(col_1hot,axis=1)\n",
    "df_bank = pd.concat([df_others,df_1hot],axis=1)\n",
    "column_order = list(df_bank)\n",
    "column_order.insert(0, column_order.pop(column_order.index('y')))\n",
    "df_bank = df_bank.loc[:, column_order]\n",
    "df_bank['y'].replace(\"no\",0,inplace=True)\n",
    "df_bank['y'].replace(\"yes\",1,inplace=True)\n",
    "df_bank['y'] = df_bank['y'].astype('category')\n",
    "\n",
    "numericcols = ['age','duration','campaign','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']\n",
    "df_num = df_bank[numericcols]\n",
    "df_stand =(df_num-df_num.min())/(df_num.max()-df_num.min())\n",
    "df_bank_categorical = df_bank.drop(numericcols,axis=1)\n",
    "df_bank = pd.concat([df_bank_categorical,df_stand],axis=1)\n",
    "df_bank.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have successfully loaded and processed both datasets. We are ready to start the ML!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Function Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get into the algorithms, let's define some helper functions that will be used across all of the models and both datasets. We will define a function to load the data (not really necessary in a Jupyter notebook, but good if this is exported as a .py for later use). We will also define a function that plots the learning curve (training and cross validation score as a function of training examples) of an estimator (classification model). Finally, we define functions to output final model scores using an untouched test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_validate, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import itertools\n",
    "import timeit\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "def import_data():\n",
    "\n",
    "    X1 = np.array(df_phish.values[:,1:-1],dtype='int64')\n",
    "    Y1 = np.array(df_phish.values[:,0],dtype='int64')\n",
    "    X2 = np.array(df_bank.values[:,1:-1],dtype='int64')\n",
    "    Y2 = np.array(df_bank.values[:,0],dtype='int64')\n",
    "    return X1, Y1, X2, Y2\n",
    "\n",
    "\n",
    "def plot_learning_curve(clf, X, y, title=\"Insert Title\"):\n",
    "    \n",
    "    n = len(y)\n",
    "    train_mean = []; train_std = [] #model performance score (f1)\n",
    "    cv_mean = []; cv_std = [] #model performance score (f1)\n",
    "    fit_mean = []; fit_std = [] #model fit/training time\n",
    "    pred_mean = []; pred_std = [] #model test/prediction times\n",
    "    train_sizes=(np.linspace(.05, 1.0, 20)*n).astype('int')  \n",
    "    \n",
    "    for i in train_sizes:\n",
    "        idx = np.random.randint(X.shape[0], size=i)\n",
    "        X_subset = X[idx,:]\n",
    "        y_subset = y[idx]\n",
    "        scores = cross_validate(clf, X_subset, y_subset, cv=10, scoring='f1', n_jobs=-1, return_train_score=True)\n",
    "        \n",
    "        train_mean.append(np.mean(scores['train_score'])); train_std.append(np.std(scores['train_score']))\n",
    "        cv_mean.append(np.mean(scores['test_score'])); cv_std.append(np.std(scores['test_score']))\n",
    "        fit_mean.append(np.mean(scores['fit_time'])); fit_std.append(np.std(scores['fit_time']))\n",
    "        pred_mean.append(np.mean(scores['score_time'])); pred_std.append(np.std(scores['score_time']))\n",
    "    \n",
    "    train_mean = np.array(train_mean); train_std = np.array(train_std)\n",
    "    cv_mean = np.array(cv_mean); cv_std = np.array(cv_std)\n",
    "    fit_mean = np.array(fit_mean); fit_std = np.array(fit_std)\n",
    "    pred_mean = np.array(pred_mean); pred_std = np.array(pred_std)\n",
    "    \n",
    "    plot_LC(train_sizes, train_mean, train_std, cv_mean, cv_std, title)\n",
    "    plot_times(train_sizes, fit_mean, fit_std, pred_mean, pred_std, title)\n",
    "    \n",
    "    return train_sizes, train_mean, fit_mean, pred_mean\n",
    "    \n",
    "\n",
    "def plot_LC(train_sizes, train_mean, train_std, cv_mean, cv_std, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Learning Curve: \"+ title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Model F1 Score\")\n",
    "    plt.fill_between(train_sizes, train_mean - 2*train_std, train_mean + 2*train_std, alpha=0.1, color=\"b\")\n",
    "    plt.fill_between(train_sizes, cv_mean - 2*cv_std, cv_mean + 2*cv_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color=\"b\", label=\"Training Score\")\n",
    "    plt.plot(train_sizes, cv_mean, 'o-', color=\"r\", label=\"Cross-Validation Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_times(train_sizes, fit_mean, fit_std, pred_mean, pred_std, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Modeling Time: \"+ title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Training Time (s)\")\n",
    "    plt.fill_between(train_sizes, fit_mean - 2*fit_std, fit_mean + 2*fit_std, alpha=0.1, color=\"b\")\n",
    "    plt.fill_between(train_sizes, pred_mean - 2*pred_std, pred_mean + 2*pred_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(train_sizes, fit_mean, 'o-', color=\"b\", label=\"Training Time (s)\")\n",
    "    plt.plot(train_sizes, pred_std, 'o-', color=\"r\", label=\"Prediction Time (s)\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(2), range(2)):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    \n",
    "def final_classifier_evaluation(clf,X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end_time = timeit.default_timer()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    start_time = timeit.default_timer()    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    end_time = timeit.default_timer()\n",
    "    pred_time = end_time - start_time\n",
    "    \n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "    print(\"Model Evaluation Metrics Using Untouched Test Dataset\")\n",
    "    print(\"*****************************************************\")\n",
    "    print(\"Model Training Time (s):   \"+\"{:.5f}\".format(training_time))\n",
    "    print(\"Model Prediction Time (s): \"+\"{:.5f}\\n\".format(pred_time))\n",
    "    print(\"F1 Score:  \"+\"{:.2f}\".format(f1))\n",
    "    print(\"Accuracy:  \"+\"{:.2f}\".format(accuracy)+\"     AUC:       \"+\"{:.2f}\".format(auc))\n",
    "    print(\"Precision: \"+\"{:.2f}\".format(precision)+\"     Recall:    \"+\"{:.2f}\".format(recall))\n",
    "    print(\"*****************************************************\")\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=[\"0\",\"1\"], title='Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The Fun Part: Machine Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will build a forward-feed neural network which computes weights via backpropagation (a multilayer perceptron). The main hyperparameter will be number of hidden nodes in a network defined by a single hidden layer, while others that could be searched over in grid search are activation function, and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def hyperNN(X_train, y_train, X_test, y_test, title):\n",
    "\n",
    "    f1_test = []\n",
    "    f1_train = []\n",
    "    hlist = np.linspace(1,150,30).astype('int')\n",
    "    for i in hlist:         \n",
    "            clf = MLPClassifier(hidden_layer_sizes=(i,), solver='adam', activation='logistic', \n",
    "                                learning_rate_init=0.05, random_state=100)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred_test = clf.predict(X_test)\n",
    "            y_pred_train = clf.predict(X_train)\n",
    "            f1_test.append(f1_score(y_test, y_pred_test))\n",
    "            f1_train.append(f1_score(y_train, y_pred_train))\n",
    "      \n",
    "    plt.plot(hlist, f1_test, 'o-', color='r', label='Test F1 Score')\n",
    "    plt.plot(hlist, f1_train, 'o-', color = 'b', label='Train F1 Score')\n",
    "    plt.ylabel('Model F1 Score')\n",
    "    plt.xlabel('No. Hidden Units')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def NNGridSearchCV(X_train, y_train):\n",
    "    #parameters to search:\n",
    "    #number of hidden units\n",
    "    #learning_rate\n",
    "    h_units = [5, 10, 20, 30, 40, 50, 75, 100]\n",
    "    learning_rates = [0.01, 0.05, .1]\n",
    "    param_grid = {'hidden_layer_sizes': h_units, 'learning_rate_init': learning_rates}\n",
    "\n",
    "    net = GridSearchCV(estimator = MLPClassifier(solver='adam',activation='logistic',random_state=100),\n",
    "                       param_grid=param_grid, cv=10)\n",
    "    net.fit(X_train, y_train)\n",
    "    print(\"Per Hyperparameter tuning, best parameters are:\")\n",
    "    print(net.best_params_)\n",
    "    return net.best_params_['hidden_layer_sizes'], net.best_params_['learning_rate_init']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phishX,phishY,bankX,bankY = import_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(phishX),np.array(phishY), test_size=0.20)\n",
    "hyperNN(X_train, y_train, X_test, y_test,title=\"Model Complexity Curve for NN (Phishing Data)\\nHyperparameter : No. Hidden Units\")\n",
    "h_units, learn_rate = NNGridSearchCV(X_train, y_train)\n",
    "estimator_phish = MLPClassifier(hidden_layer_sizes=(h_units,), solver='adam', activation='logistic', \n",
    "                               learning_rate_init=learn_rate, random_state=100)\n",
    "train_samp_phish, NN_train_score_phish, NN_fit_time_phish, NN_pred_time_phish = plot_learning_curve(estimator_phish, X_train, y_train,title=\"Neural Net Phishing Data\")\n",
    "final_classifier_evaluation(estimator_phish, X_train, X_test, y_train, y_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(bankX),np.array(bankY), test_size=0.20)\n",
    "hyperNN(X_train, y_train, X_test, y_test,title=\"Model Complexity Curve for NN (Banking Data)\\nHyperparameter : No. Hidden Units\")\n",
    "h_units, learn_rate = NNGridSearchCV(X_train, y_train)\n",
    "estimator_bank = MLPClassifier(hidden_layer_sizes=(h_units,), solver='adam', activation='logistic', \n",
    "                               learning_rate_init=learn_rate, random_state=100)\n",
    "train_samp_bank, NN_train_score_bank, NN_fit_time_bank, NN_pred_time_bank = plot_learning_curve(estimator_bank, X_train, y_train,title=\"Neural Net Banking Data\")\n",
    "final_classifier_evaluation(estimator_bank, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final section for neural network will plot the loss curve for each dataset over the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phishX,phishY,bankX,bankY = import_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(phishX),np.array(phishY), test_size=0.20)\n",
    "estimator_phish = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', \n",
    "                               learning_rate_init=0.05, random_state=100)\n",
    "estimator_phish.fit(X_train, y_train)\n",
    "a = estimator_phish.loss_curve_\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(bankX),np.array(bankY), test_size=0.20)\n",
    "estimator_bank = MLPClassifier(hidden_layer_sizes=(5,), solver='adam', activation='logistic', \n",
    "                               learning_rate_init=0.01, random_state=100)\n",
    "estimator_bank.fit(X_train, y_train)\n",
    "b = estimator_bank.loss_curve_\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(a, 'o-', color=\"b\", label=\"Phishing Data\")\n",
    "plt.plot(b, 'o-', color=\"r\", label=\"Banking Data\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will create a surface plot for the Phishing data to see where the local and global maxima occur for different numbers of hidden units for a network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# def runNeural(XTrain,YTrain,XTest,YTest,learn_rate,sz1,sz2,testit=True,times=1):\n",
    "#     for mm in range(1,times+1):\n",
    "#         #clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "#         clf = MLPClassifier(learning_rate_init=learn_rate,activation='logistic', hidden_layer_sizes=(sz1, sz2), random_state=100)\n",
    "#         clf.fit(XTrain, YTrain)\n",
    "#         if testit:\n",
    "#             mmm = clf.predict(XTest)\n",
    "#             return((sum(mmm == YTest) / len(mmm))/times)\n",
    "#     return(0)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(np.array(bankX),np.array(bankY), test_size=0.20)\n",
    "\n",
    "# trackertot2 = [[0]*5]*5\n",
    "# nspot = 10\n",
    "# trackertot = np.zeros((nspot,nspot))\n",
    "# xtot = []\n",
    "# ytot = []\n",
    "# maxtrack = 0\n",
    "# x_vals = np.linspace(1,50,nspot).astype('int')\n",
    "# y_vals = np.linspace(1,50,nspot).astype('int')\n",
    "# for x in range(1,nspot+1):\n",
    "#     xi = x_vals[x-1]\n",
    "#     for y in range(1,nspot+1):\n",
    "#         yi = y_vals[y-1]\n",
    "#         tracker = (runNeural(X_train,y_train,X_test,y_test,0.05,xi,yi))\n",
    "#         trackertot[x-1][y-1]=tracker\n",
    "#         if(tracker>maxtrack):\n",
    "#             maxtrack = tracker\n",
    "\n",
    "# X, Y = np.meshgrid(x_vals, y_vals)\n",
    "# fig = plt.figure()\n",
    "# # Add an axes\n",
    "# ax = fig.add_subplot(111,projection='3d')\n",
    "# # plot the surface\n",
    "# ax.plot_surface(X, Y, np.array(trackertot), alpha=0.2)\n",
    "\n",
    "# plt.show()\n",
    "# plt.clf()\n",
    "# plt.cla()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will build a support vector machine classifier. The primary hyperparameter will be kernel function: linear, polynomial, rbf (radial basis function), and sigmoid. We will also explore the penalty term 'C' and the kernel coefficient 'gamma'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def hyperSVM(X_train, y_train, X_test, y_test, title):\n",
    "\n",
    "    f1_test = []\n",
    "    f1_train = []\n",
    "    kernel_func = ['linear','poly','rbf','sigmoid']\n",
    "    for i in kernel_func:         \n",
    "            if i == 'poly':\n",
    "                for j in [2,3,4,5,6,7,8]:\n",
    "                    clf = SVC(kernel=i, degree=j,random_state=100)\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_pred_test = clf.predict(X_test)\n",
    "                    y_pred_train = clf.predict(X_train)\n",
    "                    f1_test.append(f1_score(y_test, y_pred_test))\n",
    "                    f1_train.append(f1_score(y_train, y_pred_train))\n",
    "            else:    \n",
    "                clf = SVC(kernel=i, random_state=100)\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred_test = clf.predict(X_test)\n",
    "                y_pred_train = clf.predict(X_train)\n",
    "                f1_test.append(f1_score(y_test, y_pred_test))\n",
    "                f1_train.append(f1_score(y_train, y_pred_train))\n",
    "                \n",
    "    xvals = ['linear','poly2','poly3','poly4','poly5','poly6','poly7','poly8','rbf','sigmoid']\n",
    "    plt.plot(xvals, f1_test, 'o-', color='r', label='Test F1 Score')\n",
    "    plt.plot(xvals, f1_train, 'o-', color = 'b', label='Train F1 Score')\n",
    "    plt.ylabel('Model F1 Score')\n",
    "    plt.xlabel('Kernel Function')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def SVMGridSearchCV(X_train, y_train):\n",
    "    #parameters to search:\n",
    "    #penalty parameter, C\n",
    "    #\n",
    "    Cs = [1e-4, 1e-3, 1e-2, 1e01, 1]\n",
    "    gammas = [1,10,100]\n",
    "    param_grid = {'C': Cs, 'gamma': gammas}\n",
    "\n",
    "    clf = GridSearchCV(estimator = SVC(kernel='rbf',random_state=100),\n",
    "                       param_grid=param_grid, cv=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Per Hyperparameter tuning, best parameters are:\")\n",
    "    print(clf.best_params_)\n",
    "    return clf.best_params_['C'], clf.best_params_['gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phishX,phishY,bankX,bankY = import_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(phishX),np.array(phishY), test_size=0.20)\n",
    "hyperSVM(X_train, y_train, X_test, y_test,title=\"Model Complexity Curve for SVM (Phishing Data)\\nHyperparameter : Kernel Function\")\n",
    "C_val, gamma_val = SVMGridSearchCV(X_train, y_train)\n",
    "estimator_phish = SVC(C=C_val, gamma=gamma_val, kernel='rbf', random_state=100)\n",
    "train_samp_phish, SVM_train_score_phish, SVM_fit_time_phish, SVM_pred_time_phish = plot_learning_curve(estimator_phish, X_train, y_train,title=\"SVM Phishing Data\")\n",
    "final_classifier_evaluation(estimator_phish, X_train, X_test, y_train, y_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(bankX),np.array(bankY), test_size=0.20)\n",
    "hyperSVM(X_train, y_train, X_test, y_test,title=\"Model Complexity Curve for SVM (Banking Data)\\nHyperparameter : Kernel Function\")\n",
    "C_val, gamma_val = SVMGridSearchCV(X_train, y_train)\n",
    "estimator_bank = SVC(C=C_val, gamma=gamma_val, kernel='rbf', random_state=100)\n",
    "estimator_bank = SVC(kernel='rbf', random_state=100)\n",
    "train_samp_bank, SVM_train_score_bank, SVM_fit_time_bank, SVM_pred_time_bank = plot_learning_curve(estimator_bank, X_train, y_train,title=\"SVM Banking Data\")\n",
    "final_classifier_evaluation(estimator_bank, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will build a classifier using K-nearest neighbors. The hyperparameter will be n_neighbors. One could easily add another hyperparameter is the distance metric, but for simplicity, and for the sake of running the code faster, I won't explore this. I will use the standard euclidean distance for all models. The Model Complexity curve will show F1 score as a function of number of neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
    "\n",
    "def hyperKNN(X_train, y_train, X_test, y_test, title):\n",
    "    \n",
    "    f1_test = []\n",
    "    f1_train = []\n",
    "    klist = np.linspace(1,250,25).astype('int')\n",
    "    for i in klist:\n",
    "        clf = kNN(n_neighbors=i,n_jobs=-1)\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        f1_test.append(f1_score(y_test, y_pred_test))\n",
    "        f1_train.append(f1_score(y_train, y_pred_train))\n",
    "        \n",
    "    plt.plot(klist, f1_test, 'o-', color='r', label='Test F1 Score')\n",
    "    plt.plot(klist, f1_train, 'o-', color = 'b', label='Train F1 Score')\n",
    "    plt.ylabel('Model F1 Score')\n",
    "    plt.xlabel('No. Neighbors')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phishX,phishY,bankX,bankY = import_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(phishX),np.array(phishY), test_size=0.20)\n",
    "hyperKNN(X_train, y_train, X_test, y_test,title=\"Model Complexity Curve for kNN (Phishing Data)\\nHyperparameter : No. Neighbors\")\n",
    "estimator_phish = kNN(n_neighbors=20, n_jobs=-1)\n",
    "train_samp_phish, kNN_train_score_phish, kNN_fit_time_phish, kNN_pred_time_phish = plot_learning_curve(estimator_phish, X_train, y_train,title=\"kNN Phishing Data\")\n",
    "final_classifier_evaluation(estimator_phish, X_train, X_test, y_train, y_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(bankX),np.array(bankY), test_size=0.20)\n",
    "hyperKNN(X_train, y_train, X_test, y_test,title=\"Model Complexity Curve for kNN (Banking Data)\\nHyperparameter : No. Neighbors\")\n",
    "estimator_bank = kNN(n_neighbors=10, n_jobs=-1)\n",
    "train_samp_bank, kNN_train_score_bank, kNN_fit_time_bank, kNN_pred_time_bank = plot_learning_curve(estimator_bank, X_train, y_train,title=\"kNN Banking Data\")\n",
    "final_classifier_evaluation(estimator_bank, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will build a Decision Tree Classifier using information gain (based on entropy) to determine the best feature split per the ID3 algorithm. The model will be pre-pruned by limiting tree depth using the hyperparameter 'max_depth' and by ensuring that each leaf (a terminal node on the tree) has at least 'min_samples_leaf'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def hyperTree(X_train, y_train, X_test, y_test, title):\n",
    "    \n",
    "    f1_test = []\n",
    "    f1_train = []\n",
    "    max_depth = list(range(1,31))\n",
    "    for i in max_depth:         \n",
    "            clf = DecisionTreeClassifier(max_depth=i, random_state=100, min_samples_leaf=1, criterion='entropy')\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred_test = clf.predict(X_test)\n",
    "            y_pred_train = clf.predict(X_train)\n",
    "            f1_test.append(f1_score(y_test, y_pred_test))\n",
    "            f1_train.append(f1_score(y_train, y_pred_train))\n",
    "      \n",
    "    plt.plot(max_depth, f1_test, 'o-', color='r', label='Test F1 Score')\n",
    "    plt.plot(max_depth, f1_train, 'o-', color = 'b', label='Train F1 Score')\n",
    "    plt.ylabel('Model F1 Score')\n",
    "    plt.xlabel('Max Tree Depth')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "     \n",
    "    \n",
    "def TreeGridSearchCV(start_leaf_n, end_leaf_n, X_train, y_train):\n",
    "    #parameters to search:\n",
    "    #20 values of min_samples leaf from 0.5% sample to 5% of the training data\n",
    "    #20 values of max_depth from 1, 20\n",
    "    param_grid = {'min_samples_leaf':np.linspace(start_leaf_n,end_leaf_n,20).round().astype('int'), 'max_depth':np.arange(1,20)}\n",
    "\n",
    "    tree = GridSearchCV(estimator = DecisionTreeClassifier(), param_grid=param_grid, cv=10)\n",
    "    tree.fit(X_train, y_train)\n",
    "    print(\"Per Hyperparameter tuning, best parameters are:\")\n",
    "    print(tree.best_params_)\n",
    "    return tree.best_params_['max_depth'], tree.best_params_['min_samples_leaf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phishX,phishY,bankX,bankY = import_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(phishX),np.array(phishY), test_size=0.20)\n",
    "hyperTree(X_train, y_train, X_test, y_test,title=\"Model Complexity Curve for Decision Tree (Phishing Data)\\nHyperparameter : Tree Max Depth\")\n",
    "start_leaf_n = round(0.005*len(X_train))\n",
    "end_leaf_n = round(0.05*len(X_train)) #leaf nodes of size [0.5%, 5% will be tested]\n",
    "max_depth, min_samples_leaf = TreeGridSearchCV(start_leaf_n,end_leaf_n,X_train,y_train)\n",
    "estimator_phish = DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf, random_state=100, criterion='entropy')\n",
    "train_samp_phish, DT_train_score_phish, DT_fit_time_phish, DT_pred_time_phish = plot_learning_curve(estimator_phish, X_train, y_train,title=\"Decision Tree Phishing Data\")\n",
    "final_classifier_evaluation(estimator_phish, X_train, X_test, y_train, y_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(bankX),np.array(bankY), test_size=0.20)\n",
    "hyperTree(X_train, y_train, X_test, y_test,title=\"Model Complexity Curve for Decision Tree (Banking Data)\\nHyperparameter : Tree Max Depth\")\n",
    "start_leaf_n = round(0.005*len(X_train))\n",
    "end_leaf_n = round(0.05*len(X_train)) #leaf nodes of size [0.5%, 5% will be tested]\n",
    "max_depth, min_samples_leaf = TreeGridSearchCV(start_leaf_n,end_leaf_n,X_train,y_train)\n",
    "estimator_bank = DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf, random_state=100, criterion='entropy')\n",
    "train_samp_bank, DT_train_score_bank, DT_fit_time_bank, DT_pred_time_bank = plot_learning_curve(estimator_bank, X_train, y_train,title=\"Decision Tree Banking Data\")\n",
    "final_classifier_evaluation(estimator_bank, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosted Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will implement a boosted version of the earlier decision tree. We will still keep the pruning based on max_depth and min_samples_leaf, but the cutoff thresholds will be more aggressive (lower) since the power of boosting is to combine multiple weak learners. We also introduce the hyperparameter of n_estimators and learning rate which will determine the contribution of each tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def hyperBoost(X_train, y_train, X_test, y_test, max_depth, min_samples_leaf, title):\n",
    "    \n",
    "    f1_test = []\n",
    "    f1_train = []\n",
    "    n_estimators = np.linspace(1,250,40).astype('int')\n",
    "    for i in n_estimators:         \n",
    "            clf = GradientBoostingClassifier(n_estimators=i, max_depth=int(max_depth/2), \n",
    "                                             min_samples_leaf=int(min_samples_leaf/2), random_state=100,)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred_test = clf.predict(X_test)\n",
    "            y_pred_train = clf.predict(X_train)\n",
    "            f1_test.append(f1_score(y_test, y_pred_test))\n",
    "            f1_train.append(f1_score(y_train, y_pred_train))\n",
    "      \n",
    "    plt.plot(n_estimators, f1_test, 'o-', color='r', label='Test F1 Score')\n",
    "    plt.plot(n_estimators, f1_train, 'o-', color = 'b', label='Train F1 Score')\n",
    "    plt.ylabel('Model F1 Score')\n",
    "    plt.xlabel('No. Estimators')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def BoostedGridSearchCV(start_leaf_n, end_leaf_n, X_train, y_train):\n",
    "    #parameters to search:\n",
    "    #n_estimators, learning_rate, max_depth, min_samples_leaf\n",
    "    param_grid = {'min_samples_leaf': np.linspace(start_leaf_n,end_leaf_n,3).round().astype('int'),\n",
    "                  'max_depth': np.arange(1,4),\n",
    "                  'n_estimators': np.linspace(10,100,3).round().astype('int'),\n",
    "                  'learning_rate': np.linspace(.001,.1,3)}\n",
    "\n",
    "    boost = GridSearchCV(estimator = GradientBoostingClassifier(), param_grid=param_grid, cv=10)\n",
    "    boost.fit(X_train, y_train)\n",
    "    print(\"Per Hyperparameter tuning, best parameters are:\")\n",
    "    print(boost.best_params_)\n",
    "    return boost.best_params_['max_depth'], boost.best_params_['min_samples_leaf'], boost.best_params_['n_estimators'], boost.best_params_['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phishX,phishY,bankX,bankY = import_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(phishX),np.array(phishY), test_size=0.20)\n",
    "hyperBoost(X_train, y_train, X_test, y_test, 3, 50, title=\"Model Complexity Curve for Boosted Tree (Phishing Data)\\nHyperparameter : No. Estimators\")\n",
    "start_leaf_n = round(0.005*len(X_train))\n",
    "end_leaf_n = round(0.05*len(X_train)) #leaf nodes of size [0.5%, 5% will be tested]\n",
    "max_depth, min_samples_leaf, n_est, learn_rate = BoostedGridSearchCV(start_leaf_n,end_leaf_n,X_train,y_train)\n",
    "estimator_phish = GradientBoostingClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf, \n",
    "                                              n_estimators=n_est, learning_rate=learn_rate, random_state=100)\n",
    "train_samp_phish, BT_train_score_phish, BT_fit_time_phish, BT_pred_time_phish = plot_learning_curve(estimator_phish, X_train, y_train,title=\"Boosted Tree Phishing Data\")\n",
    "final_classifier_evaluation(estimator_phish, X_train, X_test, y_train, y_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(bankX),np.array(bankY), test_size=0.20)\n",
    "hyperBoost(X_train, y_train, X_test, y_test, 3, 50, title=\"Model Complexity Curve for Boosted Tree (Banking Data)\\nHyperparameter : No. Estimators\")\n",
    "start_leaf_n = round(0.005*len(X_train))\n",
    "end_leaf_n = round(0.05*len(X_train)) #leaf nodes of size [0.5%, 5% will be tested]\n",
    "max_depth, min_samples_leaf, n_est, learn_rate = BoostedGridSearchCV(start_leaf_n,end_leaf_n,X_train,y_train)\n",
    "estimator_bank = GradientBoostingClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf, \n",
    "                                              n_estimators=n_est, learning_rate=learn_rate, random_state=100)\n",
    "train_samp_bank, BT_train_score_bank, BT_fit_time_bank, BT_pred_time_bank = plot_learning_curve(estimator_bank, X_train, y_train,title=\"Boosted Tree Banking Data\")\n",
    "final_classifier_evaluation(estimator_bank, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Comparison Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's define and call a function that will plot training times and learning rates for the 5 different algorithms so that we can compare across the classifiers for the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_fit_time(n,NNtime, SMVtime, kNNtime, DTtime, BTtime, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Model Training Times: \" + title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Model Training Time (s)\")\n",
    "    plt.plot(n, NNtime, '-', color=\"b\", label=\"Neural Network\")\n",
    "    plt.plot(n, SMVtime, '-', color=\"r\", label=\"SVM\")\n",
    "    plt.plot(n, kNNtime, '-', color=\"g\", label=\"kNN\")\n",
    "    plt.plot(n, DTtime, '-', color=\"m\", label=\"Decision Tree\")\n",
    "    plt.plot(n, BTtime, '-', color=\"k\", label=\"Boosted Tree\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "def compare_pred_time(n,NNpred, SMVpred, kNNpred, DTpred, BTpred, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Model Prediction Times: \" + title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Model Prediction Time (s)\")\n",
    "    plt.plot(n, NNpred, '-', color=\"b\", label=\"Neural Network\")\n",
    "    plt.plot(n, SMVpred, '-', color=\"r\", label=\"SVM\")\n",
    "    plt.plot(n, kNNpred, '-', color=\"g\", label=\"kNN\")\n",
    "    plt.plot(n, DTpred, '-', color=\"m\", label=\"Decision Tree\")\n",
    "    plt.plot(n, BTpred, '-', color=\"k\", label=\"Boosted Tree\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compare_learn_time(n,NNlearn, SMVlearn, kNNlearn, DTlearn, BTlearn, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Model Learning Rates: \" + title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Model F1 Score\")\n",
    "    plt.plot(n, NNlearn, '-', color=\"b\", label=\"Neural Network\")\n",
    "    plt.plot(n, SMVlearn, '-', color=\"r\", label=\"SVM\")\n",
    "    plt.plot(n, kNNlearn, '-', color=\"g\", label=\"kNN\")\n",
    "    plt.plot(n, DTlearn, '-', color=\"m\", label=\"Decision Tree\")\n",
    "    plt.plot(n, BTlearn, '-', color=\"k\", label=\"Boosted Tree\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_fit_time(train_samp_phish, NN_fit_time_phish, SVM_fit_time_phish, kNN_fit_time_phish, \n",
    "                 DT_fit_time_phish, BT_fit_time_phish, 'Phishing Dataset')              \n",
    "compare_pred_time(train_samp_phish, NN_pred_time_phish, SVM_pred_time_phish, kNN_pred_time_phish, \n",
    "                 DT_pred_time_phish, BT_pred_time_phish, 'Phishing Dataset')   \n",
    "compare_learn_time(train_samp_phish, NN_train_score_phish, SVM_train_score_phish, kNN_train_score_phish, \n",
    "                 DT_train_score_phish, BT_train_score_phish, 'Phishing Dataset')  \n",
    "\n",
    "\n",
    "\n",
    "compare_fit_time(train_samp_bank, NN_fit_time_bank, SVM_fit_time_bank, kNN_fit_time_bank, \n",
    "                 DT_fit_time_bank, BT_fit_time_bank, 'Banking Dataset')       \n",
    "compare_pred_time(train_samp_bank, NN_pred_time_bank, SVM_pred_time_bank, kNN_pred_time_bank, \n",
    "                 DT_pred_time_bank, BT_pred_time_bank, 'Banking Dataset')           \n",
    "compare_learn_time(train_samp_bank, NN_train_score_bank, SVM_train_score_bank, kNN_train_score_bank, \n",
    "                 DT_train_score_bank, BT_train_score_bank, 'Banking Dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
